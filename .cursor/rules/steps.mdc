---
description: 
globs: 
alwaysApply: true
---
Okay, this detailed plan is now specifically tailored for a Three.js (Rendering) + Rapier (Physics) + Socket.io (Networking) stack, using plain JavaScript, focusing on a 1v1 FPS structure, and incorporating the level of detail from your example.

No code should be written directly from this plan; it serves as a conceptual guide for an AI.

Project Plan: 1v1 FPS Duel (Three.js + Rapier + Socket.io - Detailed JS)

Core Technologies:

Client-Side: Three.js (WebGL Rendering ONLY), Rapier (Client-Side Physics: Prediction & Collision), Socket.io-client (Networking), Vanilla JavaScript (Logic, DOM Manipulation, ES Modules), HTML/CSS (UI Shell/HUD).

Server-Side: Node.js, Socket.io (Networking), Rapier (Authoritative Server-Side Physics: Validation & Simulation), Plain JavaScript (Server Logic, ES Modules or CommonJS), Potentially Redis (Queue/Session), MongoDB (Persistence).

Shared: JavaScript Modules (Common constants, utility functions, map geometry data structures, message type definitions).

Phase 1: Foundation Setup

Objective: Create the skeleton that all other systems will plug into.

1.1 Project Infrastructure

Goal: Establish a robust, maintainable, and scalable JavaScript project structure.

1.1.1 Set up monorepo with workspaces (client/server/shared)

Step 1: Initialize a new project directory.

Step 2: Choose and configure a monorepo management tool suitable for JavaScript projects (e.g., Yarn Workspaces, PNPM Workspaces, or Turborepo). Initialize it according to its documentation.

Step 3: Create three primary package directories: packages/client, packages/server, packages/shared.

Step 4: Initialize each directory as a distinct package (npm init -y or yarn init -y). Define basic package.json files. Mark them private if not intended for publishing.

Step 5: Configure the root package.json or the monorepo tool's config file (pnpm-workspace.yaml, turbo.json) to recognize these directories as workspaces.

Why: Centralizes code, simplifies dependency management (especially for shared), and streamlines build/test commands across client, server, and shared codebases using standard JavaScript tooling.

Unit Test: Verify the monorepo tool lists the workspaces. Install a dependency from the root (e.g., eslint); check it's accessible or linked correctly in sub-packages. Run a command from the root targeting a workspace (e.g., yarn workspace client lint - assuming a placeholder lint script exists).

STOP & CHECK: Confirm monorepo structure is recognized by the chosen tool and basic cross-workspace command execution works.

1.1.2 Configure module resolution/paths for cross-package imports

Step 1: Decide on JS module system (ES Modules recommended for modern JS, CommonJS might be default in Node.js). Ensure package.json ("type": "module") and file extensions (.js or .mjs) are consistent.

Step 2: For cleaner imports (avoiding ../../shared/src/utils), configure path aliases if using a bundler (like Vite, Webpack) for the client, or use Node.js Subpath Imports ("imports" field in package.json) for server/shared if using ESM. Example (package.json): "imports": { "#shared/*": "./packages/shared/src/*.js" }. If not using these, rely on relative paths initially.

Step 3: Set up ESLint with appropriate plugins (e.g., eslint-plugin-import) to enforce module import conventions and potentially check resolved paths.

Why: Enables cleaner, more maintainable imports between client, server, and shared packages using logical paths instead of fragile relative paths. The shared package is vital for common constants, utilities, and potentially data structure definitions (like message types).

Unit Test: Create a simple constant or function in shared/src/constants.js. Attempt to import and use it in client/src/main.js and server/src/server.js using the configured path alias (if any) or relative path. Run ESLint to check for unresolved imports. If using Node.js, run node server/src/server.js to ensure the import resolves at runtime. If using a bundler for the client, build it and check for errors.

STOP & CHECK: Confirm that JavaScript modules from the shared package can be successfully imported and used in both client and server code, resolving correctly during linting and runtime/build time.

1.1.3 Create CI/CD pipeline for testing/builds

Step 1: Choose CI/CD platform (GitHub Actions, GitLab CI, Jenkins).

Step 2: Create the basic configuration file (e.g., .github/workflows/ci.yml).

Step 3: Define triggers (e.g., push/pull request to main).

Step 4: Define initial jobs:

Setup: Checkout code, setup Node.js, configure package manager, install dependencies (npm ci, yarn install --frozen-lockfile, pnpm install --frozen-lockfile).

Linting: Run ESLint across all workspaces (eslint . or using monorepo tool filtering).

Testing: Run unit tests (placeholder initially, e.g., using Jest, Vitest) across relevant workspaces (npm test, yarn test, pnpm test).

Building: Run build process for client (if using a bundler) and potentially server (if transpilation is needed, though likely not for modern Node.js/JS) (npm run build, etc.).

Why: Automates quality checks (linting, testing) and build generation, ensuring consistent environments and catching issues early.

Unit Test: Create placeholder scripts in workspace package.json files (e.g., test: "echo \"Tests OK\" && exit 0"). Commit CI config. Push. Verify pipeline triggers, installs deps, and runs placeholder lint/test/build steps successfully.

STOP & CHECK: Confirm the CI pipeline runs automatically and completes defined steps without errors.

1.2 Core Rendering System (Three.js)

Goal: Establish the basic 3D environment rendering capability on the client using Three.js.

1.2.1 Initialize Three.js scene with WebGL renderer

Step 1: In the client package, npm install three.

Step 2: Create a main rendering module (e.g., client/src/rendering/GameView.js).

Step 3: Within this module, create functions to initialize:

const renderer = new THREE.WebGLRenderer({ antialias: true });

const scene = new THREE.Scene();

const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000); Set initial position/lookAt.

Step 4: Get a reference to a container element in your index.html (e.g., <div id="game-canvas-container"></div>). Use vanilla JS DOM manipulation to append the renderer's canvas: document.getElementById('game-canvas-container').appendChild(renderer.domElement);.

Step 5: Implement the render loop function: function renderLoop() { requestAnimationFrame(renderLoop); /* ... other updates ... */ renderer.render(scene, camera); }. Start the loop. Ensure cleanup logic exists if the view can be destroyed.

Why: Sets up the fundamental Three.js components (renderer, scene, camera) required to display anything in 3D. The render loop continuously draws the scene. Three.js will ONLY handle rendering; all physics/collisions are delegated to Rapier.

Unit Test: Run the client application. Verify a <canvas> element appears inside the container div. Check browser console for WebGL errors. Add a THREE.Mesh(new THREE.BoxGeometry(), new THREE.MeshBasicMaterial({ color: 0xff0000 })) to the scene and position the camera to see it. Confirm the red cube is visible. Use browser dev tools to check the render loop is active (e.g., frame rate meter).

STOP & CHECK: Confirm Three.js renderer initializes, attaches to DOM, render loop runs, and a basic object can be rendered.

1.2.2 Create viewport management system (resize handling)

Step 1: Identify the canvas container element (e.g., #game-canvas-container).

Step 2: Add a resize event listener to the window object using vanilla JS: window.addEventListener('resize', handleResize);.

Step 3: Define the handleResize function:

Get container's current clientWidth and clientHeight.

Update renderer size: renderer.setSize(width, height);.

Update camera aspect ratio: camera.aspect = width / height;.

Apply camera changes: camera.updateProjectionMatrix();.

Step 4: Ensure handleResize is called once initially to set the correct starting size. Implement cleanup (window.removeEventListener) if the view can be unmounted.

Why: Keeps the 3D rendering correctly sized and proportioned within its container, preventing distortion when the window size changes.

Unit Test: Run client with a visible 3D object. Resize the browser window. Verify the canvas resizes correctly. Verify the 3D object maintains its proportions (doesn't stretch/squash).

STOP & CHECK: Confirm resizing the window dynamically updates the Three.js renderer and camera, maintaining a correct view.

1.2.3 Set up basic lighting/shadow system

Step 1: Add lights to the scene:

const ambientLight = new THREE.AmbientLight(0x404040, 1); scene.add(ambientLight);

const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8); directionalLight.position.set(5, 10, 7); directionalLight.castShadow = true; scene.add(directionalLight);

Step 2: Configure directional light shadows: directionalLight.shadow.mapSize.width = 1024; directionalLight.shadow.mapSize.height = 1024; (adjust size for quality/perf). Configure shadow camera frustum if needed.

Step 3: Enable shadows in the renderer: renderer.shadowMap.enabled = true; renderer.shadowMap.type = THREE.PCFSoftShadowMap; (optional type).

Step 4: Create a ground plane mesh using THREE.PlaneGeometry and THREE.MeshStandardMaterial (reacts to light). Add it to the scene. Set groundPlane.receiveShadow = true;.

Step 5: For any object that should cast shadows (e.g., a test cube), use MeshStandardMaterial and set object.castShadow = true;.

Why: Lighting makes the scene visually understandable. Shadows provide depth. MeshStandardMaterial enables realistic interaction with lights and shadows.

Unit Test: Add a cube above the ground plane. Position the light. Run the client. Verify the cube and plane are lit (not flat colors). Verify the cube casts a shadow on the plane. Verify ambient light provides some visibility in shadowed areas.

STOP & CHECK: Confirm basic lighting illuminates the scene and shadows are cast and received correctly.

1.3 Physics Engine Setup (Rapier)

Goal: Establish the core physics simulation environment using Rapier on both client and server.

1.3.1 Initialize identical Rapier Worlds (client/server)

Step 1: Install the appropriate Rapier JS bindings package (@dimforge/rapier3d-compat or @dimforge/rapier3d) in both client and server packages.

Step 2: In both client/src/physics/PhysicsEngine.js and server/src/physics/PhysicsEngine.js (or similar modules), import Rapier and initialize it (likely asynchronous): import RAPIER from '@dimforge/rapier3d-compat'; await RAPIER.init();.

Step 3: Create a Rapier World instance in both environments: const gravity = { x: 0.0, y: -9.81, z: 0.0 }; const world = new RAPIER.World(gravity);. Store the world instance.

Why: Sets up the independent physics simulation environments. The server's world is the authority, the client's is used for prediction. Identical setup (gravity, etc.) is crucial.

Unit Test: Run the client initialization. Run the server initialization. Verify Rapier initializes without errors in both consoles. Check that world instances are created.

STOP & CHECK: Confirm Rapier is initialized and a World instance exists on both client and server.

1.3.2 Load shared map physics geometry (client/server)

Step 1: Define the map's static collision geometry (ground, walls, platforms) in a format Rapier understands (e.g., heightfield data, triangle mesh data - vertices and indices). Export this data from a 3D modeling tool or define it manually.

Step 2: Store this geometry data in the shared package (e.g., shared/src/mapData.js exporting arrays of vertices/indices or heightfield parameters).

Step 3: In both the client and server physics initialization logic, import the mapData.

Step 4: Use the Rapier API to create static Collider objects representing the map geometry and add them to the world. Example (for a triangle mesh): const vertices = ...; const indices = ...; const rigidBodyDesc = RAPIER.RigidBodyDesc.fixed(); const rigidBody = world.createRigidBody(rigidBodyDesc); const colliderDesc = RAPIER.ColliderDesc.trimesh(vertices, indices); world.createCollider(colliderDesc, rigidBody);. Ensure identical colliders are created on client and server from the same shared data.

Why: Critical for consistent collision detection. The server needs the authoritative map boundaries for validation, and the client needs the identical geometry for accurate prediction. Using shared data ensures consistency.

Unit Test: (Requires debug visualization or querying). After initialization on both client and server, query the Rapier world for colliders (e.g., world.colliders.forEach(...)). Log the types and positions. Verify that colliders corresponding to the map geometry exist in both simulations. If using Rapier's debug renderer on the client, verify the map colliders appear correctly.

STOP & CHECK: Confirm identical static map collision geometry is loaded into both the client and server Rapier worlds using shared data.

1.3.3 Setup physics simulation loop (world.step())

Step 1: On the server, determine the desired physics tick rate (e.g., 60Hz = ~16.67ms interval). Use setInterval to call world.step() repeatedly at this rate: const physicsInterval = 1000 / 60; setInterval(() => { world.step(); }, physicsInterval);.

Step 2: On the client, integrate the physics step into the main requestAnimationFrame render loop. Calculate the time delta since the last frame (deltaTime). Call world.step() within the loop, potentially passing the deltaTime if needed by Rapier configuration or sub-stepping logic (though often fixed steps synchronized with server are preferred for prediction). function renderLoop() { const now = performance.now(); const deltaTime = (now - lastTime) / 1000; lastTime = now; world.step(); /* ... rendering ... */ requestAnimationFrame(renderLoop); }. Note: Careful synchronization between client step time and server step time is needed for accurate prediction/reconciliation later.

Why: Advances the physics simulation forward in time, calculating movement, collisions, and resolving constraints based on forces and velocities.

Unit Test: Add a dynamic RigidBody with a Collider (e.g., a sphere) above the map's ground collider in both client and server worlds. Add logging inside the step loops. Run both. Verify the step loops are running. Observe logs or use debug rendering on the client to see the sphere fall due to gravity and stop when it collides with the ground collider defined in 1.3.2.

STOP & CHECK: Confirm the Rapier physics simulation loop runs on both client and server, and basic physics interactions (gravity, collision with static map) occur.

1.4 Network Foundation (Socket.io)

Goal: Establish the core communication layer between the client and server using Socket.io.

1.4.1 Establish WebSocket connection protocol (Socket.io)

Step 1: Install Socket.io: npm install socket.io in server package, npm install socket.io-client in client package.

Step 2: In the server package (server/src/server.js), initialize Socket.io, attaching it to a Node.js HTTP server: import { createServer } from 'http'; import { Server } from 'socket.io'; const httpServer = createServer(); const io = new Server(httpServer, { /* options */ });. Start the HTTP server: httpServer.listen(PORT);.

Step 3: Implement server-side event handlers: io.on('connection', (socket) => { console.log('Client connected:', socket.id); socket.on('disconnect', () => { console.log('Client disconnected:', socket.id); }); socket.on('message', (data) => { /* Handle generic message or specific events */ }); });.

Step 4: In the client package (e.g., client/src/network/NetworkManager.js), connect to the server: import { io } from 'socket.io-client'; const socket = io('ws://localhost:PORT');.

Step 5: Implement basic client-side event handlers: socket.on('connect', () => { console.log('Connected to server:', socket.id); }); socket.on('disconnect', () => { console.log('Disconnected from server'); }); socket.on('connect_error', (err) => { console.error('Connection error:', err); }); socket.on('message', (data) => { /* Handle messages */ });.

Why: Uses Socket.io library for robust WebSocket communication, handling connections, disconnections, and providing a structured way to send/receive named events.

Unit Test: Start server. Start client. Check server log for 'Client connected'. Check client console for 'Connected to server'. Stop client; check server log for 'Client disconnected'. Stop server; check client console for 'Disconnected from server' or 'Connection error'.

STOP & CHECK: Confirm client reliably connects to the Socket.io server, and connection/disconnection events are logged on both ends.

1.4.2 Create message serialization/deserialization layer

Step 1: In the shared package (e.g., shared/src/networkMessages.js), define constants or simple objects representing message types: const MessageType = { PING: 'ping', PONG: 'pong', PLAYER_INPUT: 'player_input', GAME_STATE: 'game_state' };. Define the expected structure of message payloads as plain JavaScript objects (documented via comments or JSDoc). Example: { type: MessageType.PLAYER_INPUT, sequence: 123, keys: { W: true, A: false, ... }, lookDir: { x, y, z }, deltaTime: 0.016 }.

Step 2: Socket.io handles JSON serialization/deserialization by default. Ensure data sent via socket.emit('eventName', payloadObject) and received in listeners socket.on('eventName', (payloadObject) => {...}) are standard JavaScript objects.

Step 3: Implement a central message handler function or use separate handlers for specific named events on both client and server. Server: socket.on(MessageType.PLAYER_INPUT, handlePlayerInput);. Client: socket.on(MessageType.GAME_STATE, handleGameState);. Inside handlers, access properties of the received payloadObject. Add basic validation if needed (checking required properties exist).

Why: Defines a clear contract for data exchange using structured JS objects and named Socket.io events, making code more readable and maintainable than handling raw messages. JSON serialization is handled implicitly.

Unit Test: Define MessageType.ECHO_REQUEST and MessageType.ECHO_RESPONSE. Server: socket.on(MessageType.ECHO_REQUEST, (data) => { socket.emit(MessageType.ECHO_RESPONSE, data); });. Client: socket.emit(MessageType.ECHO_REQUEST, { text: 'hello' }); socket.on(MessageType.ECHO_RESPONSE, (data) => { console.log('Echo received:', data); });. Run both. Send from client. Verify server logs reception (if added) and client logs the received echo object { text: 'hello' }.

STOP & CHECK: Confirm structured JS objects can be sent via named Socket.io events and received correctly on the other end, routed to appropriate handlers.

1.4.3 Implement basic heartbeat/ping system

Step 1: Use the MessageType.PING and MessageType.PONG defined earlier.

Step 2: On the client, use setInterval (e.g., every 5s): setInterval(() => { const pingData = { timestamp: Date.now() }; socket.emit(MessageType.PING, pingData); /* Start/reset disconnect timeout here */ }, 5000);.

Step 3: On the server: socket.on(MessageType.PING, (pingData) => { socket.emit(MessageType.PONG, pingData); /* Optionally track last seen time */ });.

Step 4: On the client: socket.on(MessageType.PONG, (pongData) => { const rtt = Date.now() - pongData.timestamp; console.log('Pong received, RTT:', rtt); /* Clear disconnect timeout here */ });.

Step 5: On the client, implement the disconnection timeout logic: Set a setTimeout (e.g., 15s). If the 'pong' handler clears it before it fires, connection is alive. If timeout fires, assume disconnection (handleDisconnectLogic()). Restart timeout after sending ping or receiving pong.

Step 6: (Optional Server Check): Server can track socket.lastSeenTime = Date.now() on any message and periodically iterate connected sockets, disconnecting those not seen for >30s.

Why: Detects unresponsive connections (frozen client/server, network partition) that might not trigger immediate 'disconnect' events. Ensures resources are cleaned up and connection state is accurate.

Unit Test: Run client/server. Observe logs for periodic PING/PONG exchange and RTT calculation. Use network tools to block traffic; verify client eventually logs disconnection due to timeout. Restore traffic; verify heartbeat resumes (may require reconnection logic). If server check implemented, verify server disconnects client after inactivity period.

STOP & CHECK: Confirm heartbeat is active via PING/PONG, RTT is measurable, and unresponsive connections are detected and handled by timeout logic.

Phase 2: Player Systems

Objective: Create the core interaction layer allowing players to exist and move within the world using Rapier physics.

2.1 Avatar System (Three.js & JS Logic)

Goal: Provide visual representation for players in the game world.

2.1.1 GLB character loader with animation mapping

Step 1: Obtain character GLB file(s) with animations (idle, run, jump, etc.). Place in client assets.

Step 2: Use Three.js GLTFLoader in a client module (e.g., client/src/rendering/CharacterLoader.js). Load asynchronously.

Step 3: Extract gltf.scene (the mesh group) and gltf.animations (array of AnimationClip).

Step 4: Create const mixer = new THREE.AnimationMixer(gltf.scene);.

Step 5: Store animation clips by name: const animations = {}; gltf.animations.forEach(clip => { animations[clip.name] = mixer.clipAction(clip); });.

Step 6: Implement functions to manage animations: playAnimation(name), crossFadeTo(name, duration). Example: function playAnimation(name) { // Stop others, reset and play 'name' action }.

Step 7: Add the gltf.scene object to the main Three.js scene.

Step 8: In the main render loop, update the mixer: mixer.update(deltaTime);.

Why: Loads the visual character model and sets up the Three.js animation system to control its movements based on game state.

Unit Test: Load the GLB. Verify model appears correctly. Verify animations object contains expected animation names. Call playAnimation('idle'); see idle animation. Call playAnimation('run'); see run animation. Ensure mixer.update() is called. Check console for errors.

STOP & CHECK: Confirm character model loads, renders, and basic animations can be played via JavaScript calls.

2.1.2 First-person camera rig (arms/weapon view)

Step 1: Create separate fpsScene = new THREE.Scene() and fpsCamera = new THREE.PerspectiveCamera(...). Configure fpsCamera near plane very small (e.g., 0.01).

Step 2: Load first-person arms/weapon GLB model using GLTFLoader. Add its scene (armsWeaponModel) to fpsScene.

Step 3: Position fpsCamera and armsWeaponModel relative to each other in fpsScene for the desired view.

Step 4: In main render loop, after renderer.render(mainScene, mainCamera): call renderer.clearDepth(); then renderer.render(fpsScene, fpsCamera);.

Step 5: Link mainCamera's rotation (updated by mouse look in 2.2.2) to fpsCamera.rotation: fpsCamera.quaternion.copy(mainCamera.quaternion);.

Why: Renders arms/weapon reliably on top of the world view without Z-fighting, allowing independent FOV control. Linked rotation ensures the view follows player aim.

Unit Test: Load/display the arms/weapon model. Verify it overlays the main scene. Check clearDepth() prevents world clipping through it. Rotate the main camera (manually or via 2.2.2); verify the arms/weapon view rotates identically.

STOP & CHECK: Confirm first-person arms/weapon render correctly in the foreground and follow the main camera's orientation.

2.1.3 Third-person spectator camera

Step 1: Create a JS camera controller module (e.g., ThirdPersonCamera.js). It holds state like distance, angles, target object (player model).

Step 2: Implement logic to calculate the desired camera position based on target position, distance, and orbit angles (controlled by mouse drag or keys). camera.position.set(...).

Step 3: Make the camera look at the target: camera.lookAt(target.position);.

Step 4: Implement basic occlusion handling: Perform a Three.js Raycaster check from the target's position towards the desired camera position. If it hits map geometry (need meshes for map visuals separate from Rapier colliders, or use Rapier query) before reaching the camera distance, move the camera closer to the target along the ray until just before the hit point.

Step 5: Add a JS mechanism (key press, UI button) to toggle a global isFirstPersonView flag. In the render loop, use either the first-person setup (main camera fixed relative to player physics body + FPS overlay) or the third-person setup (main camera controlled by ThirdPersonCamera module) based on this flag.

Why: Provides alternative view for spectating or debugging. Occlusion handling prevents camera clipping.

Unit Test: Implement toggle. Verify switching views works. In third-person, check camera follows the player model (even if static). Test orbit controls. Place large object; verify camera moves closer to avoid clipping.

STOP & CHECK: Confirm functional third-person camera with orbiting and basic occlusion handling is available via a toggle.

2.2 Movement Engine (Vanilla JS, Rapier)

Goal: Implement character movement driven by Rapier physics and controlled by player input.

2.2.1 WASD keyboard input handler & Rapier Force Application

Step 1: Add keydown/keyup event listeners in client JS. Maintain a state object keysPressed = { W: false, A: false, S: false, D: false, Shift: false, Space: false }. Update state on events.

Step 2: In the client's main update loop (before world.step() if prediction logic runs there, or in input collection phase):

Get the player's Three.js camera's forward direction (ignoring pitch) using camera.getWorldDirection(vector). Project onto XZ plane and normalize for horizontal direction. Calculate the right direction (cross product with Up vector).

Calculate desired velocity change based on keysPressed: W -> move along forward, A -> move along -right, etc. Scale by movement speed, potentially higher if Shift is pressed. Handle jumping intent if Space is pressed and player is grounded (check via Rapier raycast down or collision state).

Step 3: CRITICAL: Get a reference to the player's Rapier RigidBody (created later or assumed here). Apply forces or directly set velocity on the Rapier body based on the desired movement. For character controllers, use methods like kinematicCharacterController.computeColliderMovement() or apply impulses/velocity changes to a dynamic RigidBody. Example (simplified velocity): const currentVel = playerBody.linvel(); playerBody.setLinvel({ x: desiredVelX, y: currentVel.y, z: desiredVelZ }, true);. Jumping might involve playerBody.applyImpulse({ x: 0, y: jumpForce, z: 0 }, true);.

Why: Captures input and translates it into physical forces/velocity changes within the Rapier simulation, which will then handle movement and collisions.

Unit Test: (Requires player body in Rapier - see 2.3). Add logs for key states and calculated desired velocity/force. Press WASD. Verify logs show correct intent. Observe player Rapier body (via debug render or logging state): verify it moves according to input forces/velocities within the Rapier world. Test jump impulse.

STOP & CHECK: Confirm WASD/Jump inputs are translated into appropriate force/velocity changes applied to the player's Rapier physics body.

2.2.2 Mouse look controller & Camera Sync

Step 1: Add mousemove listener. Request Pointer Lock on canvas click (canvasElement.requestPointerLock()).

Step 2: In mousemove handler (when pointer lock active), use event.movementX and event.movementY.

Step 3: Maintain pitch and yaw angles in JavaScript state. Update yaw based on movementX * sensitivity. Update pitch based on movementY * sensitivity. Clamp pitch (e.g., pitch = Math.max(-Math.PI / 2, Math.min(Math.PI / 2, pitch))).

Step 4: Update the Three.js mainCamera's rotation using Euler angles or Quaternions derived from the pitch and yaw angles. Ensure the order is correct (usually Yaw then Pitch). mainCamera.rotation.set(pitch, yaw, 0, 'YXZ');.

Step 5: The calculated camera direction (used in 2.2.1 for WASD) should read from this updated mainCamera orientation.

Why: Implements standard FPS mouse aiming. Pointer lock provides raw mouse delta. Updates the visual camera orientation. Note: Player physics body rotation might be updated separately based on yaw, or only the view direction changes.

Unit Test: Activate pointer lock. Move mouse. Verify Three.js camera view updates correctly (pan/tilt). Check pitch clamping. Press W; verify movement direction uses the new camera orientation.

STOP & CHECK: Confirm mouse movement controls Three.js camera pitch/yaw, pointer lock works, and WASD directionality follows camera view.

2.2.3 Collision detection & Response (via Rapier)

Step 1: Collision Detection is now implicit: It happens automatically when world.step() is called on client and server, based on the Collider shapes (player, map) defined in Rapier.

Step 2: Collision Response is also handled by Rapier: When the player's Rapier body (driven by forces from 2.2.1) attempts to move into a static map collider (from 1.3.2), Rapier's solver prevents interpenetration and calculates the resulting motion (stopping, sliding).

Step 3: Gravity: Already applied as a world force in Rapier (new RAPIER.World({ x:0, y:-9.81, z:0 })). Rapier handles applying this force constantly.

Step 4: Ground Check: To know if jumping is allowed, perform a Rapier raycast or shape-cast downwards from the player body's position just before applying the jump impulse. const ray = new RAPIER.Ray({ x, y, z }, { x: 0, y: -1, z: 0 }); const maxToi = 0.1; // Small distance check const hit = world.castRay(ray, maxToi, true); if (hit) { isGrounded = true; }. Alternatively, check collision events if using character controller.

Step 5: Update Visuals: After world.step() completes in the client's loop, read the updated position and rotation from the player's Rapier RigidBody: const physPos = playerBody.translation(); const physRot = playerBody.rotation();. Apply these values to the player's Three.js visual mesh (playerMesh.position.set(physPos.x, physPos.y, physPos.z); playerMesh.quaternion.set(physRot.x, physRot.y, physRot.z, physRot.w);).

Why: Leverages the physics engine (Rapier) to handle all complex collision detection and response calculations accurately and efficiently, ensuring physically plausible interactions with the environment. Visuals are synchronized after physics simulation.

Unit Test: (Requires player body). Drive player Rapier body towards a map collider using forces from 2.2.1. Step the Rapier world. Use debug render or log Rapier body position. Verify the body stops/slides along the map collider as expected by physics. Walk off ledge; verify body falls due to gravity. Check ground raycast logic correctly reports grounded status. Verify the Three.js mesh accurately follows the Rapier body's position/rotation each frame.

STOP & CHECK: Confirm Rapier handles collisions between the player body and map geometry automatically. Gravity affects the player. Ground checks work. Three.js visuals accurately reflect the state of the Rapier physics body after each step.

2.3 Networked State (Socket.io, Rapier, JS Logic)

Goal: Synchronize player state driven by Rapier physics across the network.

2.3.1 Client prediction/reconciliation (Rapier based)

Step 1: Client Input Buffer: Store client inputs (key states, look direction quaternion, delta time) locally with an incrementing sequence number. const inputs = []; let sequence = 0;.

Step 2: Client Prediction: In the client loop, apply the latest input to the local player's Rapier body (using forces/velocity as in 2.2.1) and step the local Rapier world (world.step()). Update the Three.js visuals based on the predicted Rapier state (as in 2.2.3).

Step 3: Send Inputs: Periodically (e.g., 20 times/sec), send buffered inputs (with sequence numbers) to the server via Socket.io: socket.emit(MessageType.PLAYER_INPUT, { sequence: sequence++, input: currentInput, deltaTime: dt });. Store sent inputs temporarily.

Step 4: Receive Server State: Listen for authoritative state updates from server: socket.on(MessageType.GAME_STATE, (serverState) => { /* Handle reconciliation */ });. Server state should include position, rotation, velocity for the player, and the lastProcessedSequenceNumber.

Step 5: Reconciliation:

Find the received serverState corresponding to the local player.

Set the local player's Rapier body state directly to the received server state: playerBody.setTranslation(serverPos, true); playerBody.setRotation(serverRot, true); playerBody.setLinvel(serverVel, true);.

Discard any locally stored sent inputs with sequence numbers <= serverState.lastProcessedSequenceNumber.

Re-apply the remaining sent inputs (those newer than acknowledged) sequentially onto the corrected Rapier state by simulating multiple Rapier steps instantly: remainingInputs.forEach(input => { /* Apply input forces */ world.step(fixedDeltaTime); });.

Update the Three.js visuals immediately after reconciliation to reflect the corrected+re-simulated state.

Why: Provides instant responsiveness for local player movement by predicting physics locally. Corrects any divergence from the server's authoritative physics simulation via reconciliation, reapplying subsequent inputs to minimize visual jarring.

Unit Test: (Requires 2.3.2). Introduce artificial latency. Move player on client. Verify movement feels instant locally. Observe client logs: check inputs sent, server states received, acknowledged sequence numbers increasing. Log Rapier body state before/after reconciliation: verify it snaps to server state then re-simulates pending inputs. Visually check for corrections (small snaps expected under latency).

STOP & CHECK: Confirm client predicts movement using its local Rapier world, sends inputs, receives server's Rapier state, and reconciles by correcting local Rapier state and re-simulating newer inputs.

2.3.2 Server-side movement validation (Rapier based)

Step 1: On server, create and maintain an authoritative Rapier RigidBody and Collider for each connected player within the server's world. Store player state (including last processed input sequence number).

Step 2: On receiving MessageType.PLAYER_INPUT from client:

Retrieve the player's authoritative Rapier body and state.

Apply the received input forces/velocities (from input payload) to the server's Rapier body for that player.

Step the server's Rapier world (world.step()). The physics engine handles movement, collision, response based on the input. This step inherently validates movement against map geometry.

Perform additional validation: Check resulting velocity against speed limits. Check for impossible state changes (e.g., jump impulse applied when not grounded according to server raycast).

Step 3: If validation passes, update the player's authoritative state (position, rotation, velocity read from the Rapier body) and store the sequence number from the input packet as the lastProcessedSequenceNumber. If validation fails, potentially ignore the input or log violation, don't update state based on it.

Step 4: Periodically (e.g., 10-20 times/sec), gather the authoritative Rapier state (translation, rotation, linvel) and lastProcessedSequenceNumber for relevant players. Send this in a MessageType.GAME_STATE packet via Socket.io to clients.

Why: The server uses its own Rapier simulation as the ground truth. By applying client inputs within this authoritative simulation, it ensures all movement adheres to physics rules and map boundaries, preventing cheating.

Unit Test: Send valid inputs from client; log server Rapier body state; verify it updates realistically. Send invalid inputs (too fast, trying to move through server map collider); verify server Rapier simulation prevents/corrects the movement, logs violation (if implemented), and the broadcasted state doesn't reflect the invalid move. Verify state packets are broadcast containing Rapier state and sequence numbers.

STOP & CHECK: Confirm server applies client inputs to its authoritative Rapier world, validates movement implicitly via physics simulation and explicit checks, maintains authoritative state, and broadcasts Rapier state + sequence numbers.

2.3.3 State snapshot interpolation (Rapier state based)

Step 1: On client, when receiving MessageType.GAME_STATE, identify updates for other players (remote players).

Step 2: Store incoming Rapier state snapshots (position vectors, rotation quaternions) for each remote player in a timed buffer (e.g., array per player: [{ timestamp: t1, pos: p1, rot: r1 }, { timestamp: t2, pos: p2, rot: r2 }]). Keep at least the last two. Use server timestamp if provided, otherwise client receive time.

Step 3: In the client's render loop, determine the target render time (e.g., renderTime = Date.now() - interpolationDelay; where interpolationDelay is ~100ms).

Step 4: For each remote player's Three.js mesh:

Find the two state snapshots in the buffer (state1, state2) whose timestamps bracket renderTime.

Calculate interpolation factor: alpha = (renderTime - state1.timestamp) / (state2.timestamp - state1.timestamp); Clamp alpha between 0 and 1.

Interpolate Rapier state: Use Three.js Vector3.lerpVectors(state1.pos, state2.pos, alpha) for position. Use Quaternion.slerpQuaternions(state1.rot, state2.rot, alpha) for rotation.

Step 5: Apply the calculated interpolatedPosition and interpolatedRotation to the remote player's Three.js mesh (remotePlayerMesh.position.copy(interpolatedPosition); remotePlayerMesh.quaternion.copy(interpolatedRotation);).

Why: Smooths the visual representation of remote players, hiding network jitter and the discrete arrival of state updates by displaying entities moving fluidly between known past states. Uses the authoritative Rapier states as the basis for interpolation.

Unit Test: Connect two clients. Move Player A. Observe Player A on Player B's screen. Verify movement is smooth, not jerky snaps. Introduce latency/jitter; verify movement remains visually smooth (though more delayed). Log buffered states and interpolated results on Client B to confirm calculations. Check that interpolation stops correctly if updates stop arriving (extrapolation might be added later if needed).

STOP & CHECK: Confirm remote players' visual movement is smoothed using interpolation between received authoritative Rapier state snapshots.

Okay, let's continue with Phase 3, maintaining the highly detailed, step-by-step approach suitable for guiding an AI, using the Three.js (Rendering) + Rapier (Physics) + Socket.io (Networking) + Plain JavaScript stack for a 1v1 FPS duel.

Phase 3: Combat Systems

Objective: Build the actual gameplay mechanics for the 1v1 duel, focusing on shooting and damage, leveraging Rapier for physics interactions.

3.1 Weapon System

Goal: Implement the core shooting mechanics using Rapier for hit detection.

3.1.1 Raycast-based shooting mechanism (using Rapier)

Step 1: Define Weapon Properties: In the shared package (e.g., shared/src/gameConfig.js), define JavaScript objects representing weapon stats: const WEAPONS = { pistol: { fireRate: 500, // ms between shots damage: 15, range: 50, // Rapier units }, rifle: { ... } };.

Step 2: Client Fire Input: In the client's input handling module, detect the primary fire input (e.g., mousedown or specific key). Add a timestamp variable lastFireTime = 0 for the equipped weapon.

Step 3: Client Fire Logic: On fire input:

Check fire rate: if (Date.now() - lastFireTime < WEAPONS[currentWeapon].fireRate) return;. If check passes, update lastFireTime = Date.now();.

Determine Ray Origin: Get the Three.js camera's world position. const origin = camera.position.clone();. (Or a muzzle point attached to the FPV weapon model, whose position is derived from the camera).

Determine Ray Direction: Get the Three.js camera's forward direction. const direction = camera.getWorldDirection(new THREE.Vector3());.

Instant Client Feedback: Trigger immediate visual/audio effects (muzzle flash using Three.js particles/sprites, play sound using Web Audio API). Do NOT perform client-side hit detection for damage. Client-side raycasting against visuals could be done only for non-authoritative effects like bullet hole decals later, but is not core to hit registration.

Send Fire Event: Emit a Socket.io event to the server including the calculated origin and direction vectors and the player's current input sequence number (from 2.3.1). socket.emit(MessageType.PLAYER_FIRE, { origin: {x, y, z}, direction: {x, y, z}, sequence: lastSentInputSequence });.

Step 4: Server Fire Event Handler: On the server, listen for the MessageType.PLAYER_FIRE event: socket.on(MessageType.PLAYER_FIRE, (fireData) => { ... });.

Step 5: Server Validation: Inside the handler:

Retrieve the firing player's authoritative state (are they alive? do they have ammo? respect fire rate based on server time?).

Retrieve the weapon's range from config: const range = WEAPONS[player.weapon].range;.

Step 6: Server Authoritative Raycast (Rapier):

Get the player's authoritative position/orientation at the time corresponding to the input fireData.sequence (requires storing historical state or performing rewind - see 3.1.3, start simple using current server state first). Let's assume authoritativeOrigin and authoritativeDirection are determined.

Create a Rapier Ray: const ray = new RAPIER.Ray(authoritativeOrigin, authoritativeDirection);. Ensure origin and direction are converted to the format Rapier expects (e.g., {x, y, z}).

Perform the Rapier world raycast: const hit = world.castRayAndGetNormal(ray, range, true, RAPIER.QueryFilterFlags.EXCLUDE_FIXED, undefined, undefined, /* casting shape owner (player body) */ playerBody);.

range: Max distance from weapon config.

true: Query solids.

filterFlags: Specify what to hit (e.g., only dynamic bodies, exclude fixed/static map geometry - adjust as needed). Initially, you might want to hit everything to test. Modify flags/groups later to ensure players don't hit themselves, potentially ignore teammates, etc.

groups: Collision groups/filter masks can be used for fine-grained control (e.g., ray only interacts with 'player_hitbox' group).

playerBody: Pass the caster's own rigid body to potentially ignore self-hits depending on filter setup.

Process Hit Result: Check if (hit). hit.collider gives the Rapier Collider object that was hit. hit.toi gives the time of impact (distance along the ray). hit.normal gives the surface normal at the hit point.

Why: Uses the server's authoritative physics world (Rapier) to determine what the shot actually hit, preventing client-side cheats. Client gets immediate feedback for responsiveness, but server decides the outcome. Rapier's raycasting is physics-aware.

Unit Test: Client: Press fire; verify visual/audio feedback occurs instantly and PLAYER_FIRE message sent (check network tab/logs). Server: Set up a static target Rapier body/collider. Log server reception of PLAYER_FIRE. Log the origin, direction, and range used for the Rapier raycast. Log the hit result from world.castRayAndGetNormal. Fire towards the target; verify hit is not null and hit.collider corresponds to the target collider. Fire away from target; verify hit is null.

Troubleshooting:

Ray hits nothing: Check ray origin/direction calculation (Three.js coords vs Rapier coords?), check range, check raycast filter flags/groups (are they filtering out the target?), ensure target collider exists and is solid.

Ray hits self: Ensure the caster's own collider/body is correctly filtered out using filter flags/groups or by passing the owner body to castRay.

Incorrect origin/direction: Double-check coordinate system conversions if Three.js and Rapier use different conventions. Ensure camera direction is correctly transformed to world space.

STOP & CHECK: Confirm client firing triggers immediate local feedback and sends fire event. Confirm server receives event, performs authoritative Rapier raycast using appropriate origin/direction/range/filters, and correctly identifies hit colliders or misses.

3.1.2 Bullet spread/recoil patterns

Step 1: Define Spread/Recoil Parameters: Add to shared/src/gameConfig.js within weapon definitions: pistol: { ..., baseSpread: 0.01, // Radians spreadIncreasePerShot: 0.005, maxSpread: 0.05, spreadRecoveryRate: 0.02, // Radians per second visualRecoilUp: 0.1, visualRecoilSide: 0.05, recoilRecoverySpeed: 5 }.

Step 2: Manage Weapon State (Server & Client):

Server: Add currentSpread (radians) to player state, initialized to baseSpread.

Client: Add currentRecoilOffset (a THREE.Vector2 or similar for pitch/yaw offset) to camera state, initialized to zero. Add currentSpread (for potential crosshair visualization).

Step 3: Apply Spread (Server): Before the server's Rapier raycast (Step 6 in 3.1.1):

Get the current player.currentSpread.

Generate a random direction offset within a cone defined by currentSpread. This involves generating random points on a unit circle, scaling by tan(currentSpread), and adding this offset perpendicular to the authoritativeDirection. Be careful with vector math here. A simpler approximation is to slightly randomize the x/y/z components of the normalized direction vector and re-normalize.

Use this modified direction for the RAPIER.Ray.

Increase player.currentSpread: player.currentSpread = Math.min(WEAPONS[player.weapon].maxSpread, player.currentSpread + WEAPONS[player.weapon].spreadIncreasePerShot);.

Step 4: Implement Spread Recovery (Server): In the server's main update loop (e.g., setInterval), decrease currentSpread towards baseSpread over time for players who haven't fired recently: if (!player.isFiring) { player.currentSpread = Math.max(WEAPONS[player.weapon].baseSpread, player.currentSpread - WEAPONS[player.weapon].spreadRecoveryRate * serverDeltaTime); }.

Step 5: Apply Visual Recoil (Client): When the client fires (Step 3 in 3.1.1):

Calculate a recoil kick based on visualRecoilUp, visualRecoilSide, potentially with some randomness.

Directly add this kick to the client's camera pitch and yaw state variables (from 2.2.2). This causes the immediate visual jump. cameraLogic.pitch += recoilKickY; cameraLogic.yaw += recoilKickX;.

Step 6: Implement Visual Recoil Recovery (Client): In the client's render loop, smoothly decrease the applied pitch/yaw offsets back towards zero over time using the recoilRecoverySpeed and deltaTime. cameraLogic.pitch = lerp(cameraLogic.pitch, targetPitch, deltaTime * recoilRecoverySpeed);. The player fighting the recoil naturally adjusts the targetPitch/targetYaw.

Step 7: Update Client Crosshair (Optional): If displaying a dynamic crosshair, adjust its size based on the currentSpread value (which the server should include in state updates, or client can estimate).

Why: Spread adds inaccuracy controlled by the server. Visual recoil provides immediate feedback to the client, requiring them to actively compensate, adding skill. The server doesn't need to simulate client recoil directly; it just uses the aim direction the client sends, which is naturally affected by the client compensating (or not) for their visual recoil.

Unit Test: Server: Log the calculated ray direction before and after applying spread; verify it changes slightly and randomly. Fire rapidly; log currentSpread; verify it increases. Stop firing; verify currentSpread decreases. Client: Fire weapon; verify camera view kicks noticeably. Observe recoil recovery visually.

Troubleshooting:

Spread not working: Verify spread calculation logic (vector math). Ensure currentSpread state is updated correctly and used in ray direction calculation.

Recoil feels wrong: Adjust visualRecoilUp/Side and recoilRecoverySpeed values. Ensure recoil is applied directly to camera angles and recovery logic works smoothly.

Spread/Recoil state mismatch: Server must manage authoritative spread. Client manages visual recoil. Client might need currentSpread from server for crosshair.

STOP & CHECK: Confirm server applies random spread to shot direction based on weapon state. Confirm client experiences immediate visual camera recoil and recovery.

3.1.3 Hit registration (client/server sync - Rapier context)

Step 1: Lag Compensation Foundation (Server): The server needs player position history. In the server loop, after stepping the Rapier world, store the current position (body.translation()) and rotation (body.rotation()) of each player's authoritative Rapier body, along with the current server time or tick number, in a short circular buffer (e.g., last 1-2 seconds worth).

Step 2: Estimate Latency (Server): When PLAYER_FIRE arrives, estimate the round-trip time (RTT) for the firing client (e.g., using smoothed RTT from heartbeat pings). Calculate approximate one-way latency: latency = smoothedRTT / 2;.

Step 3: Determine Fire Time (Server): Calculate the approximate server time when the input was generated: fireTime = currentServerTime - latency;.

Step 4: Server-Side Rewind (Rapier Bodies): Before the authoritative raycast:

For each potential target player: Find the two position/rotation history snapshots (state1, state2) that bracket fireTime.

Interpolate between these two snapshots to estimate the target's Rapier body position and rotation at fireTime. (Use linear interpolation for position, spherical linear interpolation (slerp) for rotation).

Crucially: Temporarily move the target player's authoritative Rapier body to this calculated historical position/rotation using targetBody.setTranslation(historicalPos, true); targetBody.setRotation(historicalRot, true);. Do this for ALL potential targets.

Step 5: Perform Authoritative Raycast (Server): Now, execute the Rapier raycast (from 3.1.1, using the shooter's position also potentially rewound based on their input sequence number relative to current state) against the world where target bodies are temporarily in their historical positions.

Step 6: Restore Positions (Server): Immediately after the raycast, restore all target players' Rapier bodies back to their current authoritative positions/rotations. targetBody.setTranslation(currentPos, true); targetBody.setRotation(currentRot, true);.

Step 7: Hit Notification: If the rewound raycast confirms a hit (hit.collider belongs to a player hitbox):

Proceed to damage calculation (3.2).

Send MessageType.HIT_CONFIRMED back to the shooter (maybe include hit location info).

Why: Compensates for network latency by performing the hit check against where the server estimates the target was on the shooter's screen when they fired, rather than where the target is now. This makes shooting feel more intuitive ("favor the shooter"). Requires careful state management.

Unit Test: (Requires Hitboxes - 3.2.2). Simulate latency. Have Player A shoot at moving Player B. Log server actions: Check latency estimate. Check stored history buffer. Check calculated historical position for Player B. Log the Rapier body position of B before rewind, during raycast (should be historical), and after restore (should be current). Verify the raycast uses these rewound positions. Test edge cases: hitting where target was. Check performance impact of storing history and rewinding positions.

Troubleshooting:

Rewind inaccurate: Check history buffer size and frequency. Check interpolation logic (lerp/slerp). Check latency estimation accuracy.

Performance cost: Storing history and iterating/moving bodies can be expensive. Optimize history storage and the rewind/restore process. Only rewind players potentially in the line of sight.

State management complexity: Ensure bodies are always restored correctly, even if errors occur during the raycast.

STOP & CHECK: Confirm server stores player position history, estimates latency, temporarily rewinds target Rapier body positions to the past for raycasting, restores positions afterward, and bases hit confirmation on this lag-compensated check.

3.2 Damage Model

Goal: Define how damage is applied and tracked based on Rapier hits.

3.2.1 Health/armor management system

Step 1: Server Player State: Add health = 100; and armor = 0; (or starting value) properties to the server-side player state object.

Step 2: Define Damage Rules: Establish rules (e.g., in comments or config): Does armor have durability? Does it reduce damage by a percentage or flat amount? Example: Armor takes 50% of damage, reducing armor value first, then health takes the rest.

Step 3: Server Damage Application Function: Create a function applyDamage(targetPlayer, damageAmount) on the server.

Inside, check targetPlayer.armor. Apply damage to armor first based on rules, reducing targetPlayer.armor.

Apply remaining damage to targetPlayer.health.

Check for death: if (targetPlayer.health <= 0) { handlePlayerDeath(targetPlayer, killerPlayer); }.

Step 4: Server Hit Processing: When a server raycast confirms a hit (3.1.3) and damage amount is determined (base weapon damage + multipliers from 3.2.2): Call applyDamage(victimPlayer, finalDamageAmount);.

Step 5: Handle Player Death (Server): Create handlePlayerDeath(victim, killer) function:

Set victim.state = 'dead'; victim.health = 0;.

Increment scores (see 3.3.2).

Broadcast MessageType.PLAYER_DIED with victim/killer info.

Start a respawn timer for the victim: setTimeout(() => { respawnPlayer(victim); }, 5000);.

Step 6: Respawn Logic (Server): Create respawnPlayer(player) function:

Set player.state = 'alive'; player.health = 100; player.armor = 0; (reset stats).

Find a spawn point location.

Crucially: Move the player's authoritative Rapier body to the spawn point: playerBody.setTranslation(spawnPos, true); playerBody.setLinvel({x:0,y:0,z:0}, true); playerBody.setAngvel({x:0,y:0,z:0}, true);.

Send updated state to clients.

Step 7: State Synchronization: Include health, armor, and player.state ('alive'/'dead') in the regular MessageType.GAME_STATE updates sent to clients.

Step 8: Client Feedback:

Display received health/armor on HUD (update via vanilla JS DOM manipulation).

On taking damage (e.g., health decreases in state update), show feedback (red flash overlay, sound).

If client's own state becomes 'dead', disable input, switch camera (e.g., to spectator or death cam), show respawn timer UI. Re-enable input/switch camera back upon receiving 'alive' state after respawn.

Why: Tracks player survivability state authoritatively on the server. Defines consequences of combat and handles the death/respawn cycle. Provides necessary info for client feedback.

Unit Test: Server: Call applyDamage directly; verify health/armor decrease correctly based on rules. Call until health <= 0; verify handlePlayerDeath is called, scores incremented (check logs/state), PLAYER_DIED emitted, respawn timer started. Verify respawnPlayer resets state and moves Rapier body (check logged position). Client: Receive state updates; verify HUD shows correct health/armor. Simulate taking damage; check feedback. Simulate receiving 'dead' state; check UI changes/input disabled. Simulate respawn; check UI/input restored.

Troubleshooting:

Incorrect damage values: Double-check armor absorption logic.

Player doesn't die/respawn: Ensure health <= 0 check works, handlePlayerDeath called, timer works, respawnPlayer correctly resets state and moves Rapier body.

Client UI not updating: Check state updates are being sent and client handler correctly updates DOM elements.

STOP & CHECK: Confirm server manages health/armor, applies damage, handles death/respawn cycle including moving Rapier body, syncs state. Confirm client displays state and provides feedback.

3.2.2 Hitbox configuration per character (Rapier Colliders)

Step 1: Define Hitbox Structure: Decide on hitbox parts (e.g., head, torso, leftArm, rightArm, leftLeg, rightLeg). Define damage multipliers for each in shared/src/gameConfig.js: HITBOX_MULTIPLIERS = { head: 2.0, torso: 1.0, limb: 0.75 };.

Step 2: Create Hitbox Colliders (Server): When creating the player's authoritative physics representation on the server (in 2.3.2):

Create the main RigidBody (dynamic or kinematic).

Instead of one collider, create multiple Rapier Collider objects, each representing a hitbox. Use appropriate shapes (ColliderDesc.ball, ColliderDesc.capsule, ColliderDesc.cuboid).

Position these colliders relative to the main RigidBody to match the character's structure. ColliderDesc.translation(x, y, z) sets relative position.

Crucially: Attach user data to each collider to identify its type: colliderDesc.setUserData({ type: 'playerHitbox', part: 'head', playerId: player.id }); (or similar structure).

Set appropriate collision groups/masks (colliderDesc.setCollisionGroups(...)) so hitboxes interact with rays but perhaps not solid map geometry directly (depending on desired physics). The main body collider might handle map collision. This requires careful setup. Alternatively, use sensor colliders if they shouldn't exert physics forces.

Add all these colliders to the server's Rapier world, associated with the player's main RigidBody.

Step 3: Update Server Raycast: In the authoritative Rapier raycast (3.1.1 / 3.1.3):

Ensure the raycast's filter flags/groups are set to intersect with the player hitbox colliders.

When a hit occurs (if (hit)), get the hit collider: const hitCollider = hit.collider;.

Retrieve the user data: const userData = hitCollider.getUserData();.

Check if userData indicates it's a player hitbox: if (userData && userData.type === 'playerHitbox').

Step 4: Determine Hit Location & Multiplier:

Get the hitbox part from user data: const part = userData.part; // 'head', 'torso', 'limb'.

Get the victim player ID: const victimId = userData.playerId;. Find the victim player object.

Look up the multiplier: const multiplier = HITBOX_MULTIPLIERS[part] || 1.0;.

Step 5: Apply Damage: Calculate final damage: const finalDamage = baseWeaponDamage * multiplier;. Call applyDamage(victimPlayer, finalDamage);.

Why: Enables location-specific damage by using multiple physics colliders per player, identified via user data. Raycasting against these provides precise hit location info for applying multipliers, adding skill depth. Requires careful configuration of collider properties and collision filtering.

Unit Test: Server: Use debug visualization or extensive logging to verify hitbox colliders are created and positioned correctly relative to the main body. Verify user data is attached. Shoot at different parts of the player representation. Log the userData of the hit collider; verify it correctly identifies 'head', 'torso', 'limb', etc. Log the applied damage multiplier; verify it matches the hit location. Test shooting gaps between hitboxes (should be a miss).

Troubleshooting:

Ray misses hitboxes: Check collision groups/masks on ray and hitboxes. Check hitbox positions/sizes. Check ray filter flags.

Ray hits wrong hitbox: Verify hitbox positioning and shapes accurately represent the intended areas.

User data missing/incorrect: Ensure setUserData is called correctly during collider creation and getUserData is retrieved correctly after hit.

Physics instability: Complex multi-collider setups attached to one body can sometimes be unstable. Ensure colliders don't overlap incorrectly. Consider using composite shapes or adjusting physics solver parameters if issues arise. Mark hitboxes as sensors (isSensor = true) if they shouldn't cause physical collisions.

STOP & CHECK: Confirm multiple Rapier colliders representing hitboxes are created per player, user data identifies them, raycasts correctly hit specific hitboxes, and damage multipliers are applied based on the hit location derived from collider user data.

3.2.3 Server-side damage calculation (Consolidation)

Step 1: Code Review: Examine all code related to shooting and damage processing (PLAYER_FIRE handler, raycasting, hitbox checking, health/armor updates).

Step 2: Verify Authority: Confirm unequivocally that all steps determining the outcome of a shot happen exclusively on the server. This includes:

Validating the shot request (fire rate, ammo, alive status).

Performing lag compensation / state rewinding.

Executing the authoritative Rapier raycast.

Identifying the hit collider and its user data (hitbox part).

Calculating base damage (weapon stats).

Applying hitbox multipliers.

Applying damage falloff based on distance (hit.toi).

Calculating armor absorption.

Updating victim's authoritative health/armor state.

Triggering death/respawn logic.

Step 3: Client Role Confirmation: Confirm the client's role is limited to:

Sending fire input (origin, direction, sequence number).

Showing immediate non-authoritative feedback (muzzle flash, sound).

Receiving authoritative state updates from the server (HIT_CONFIRMED, GAME_STATE with health/armor, PLAYER_DIED).

Displaying information and feedback based only on received server messages.

Why: Absolutely critical for cheat prevention and competitive integrity. Any damage logic on the client can be easily bypassed or manipulated. Centralizing on the server ensures fairness and consistency.

Unit Test: Conceptual code review is the primary test. Add assertions or logs on the server to confirm damage calculations only use server-side data. Attempt (conceptually) to modify client code to send fake HIT_CONFIRMED or DEAL_DAMAGE messages; verify the server has no handlers for such messages or explicitly ignores them. Verify client health only changes when a valid GAME_STATE update is received from the server.

Troubleshooting:

Suspected client cheating: If damage seems inconsistent, meticulously trace the server-side calculation path from PLAYER_FIRE input to health update, ensuring no client data other than the initial input affects the outcome.

STOP & CHECK: Confirm 100% of damage calculation logic resides and executes solely on the server, using server-authoritative data and physics results.

3.3 Match Flow (1v1 Focus)

Goal: Structure the 1v1 duel into defined rounds with clear start/end conditions.

3.3.1 Round timer/countdown system

Step 1: Server Match State: Define match states (constants/enum) in server/src/matchManager.js or similar: const MatchState = { WAITING: 'waiting', COUNTDOWN: 'countdown', IN_PROGRESS: 'in_progress', ROUND_OVER: 'round_over', MATCH_OVER: 'match_over' };. Store current state per match instance: let currentMatchState = MatchState.WAITING;. Store round duration: const ROUND_DURATION_SECONDS = 120;.

Step 2: Server Countdown Logic: When both players connect to the match instance:

Set currentMatchState = MatchState.COUNTDOWN;.

Start a countdown timer: let countdown = 5; const countdownInterval = setInterval(() => { countdown--; broadcastMatchState(); if (countdown === 0) { clearInterval(countdownInterval); startRound(); } }, 1000);.

Implement broadcastMatchState() to send current state and remaining time (countdown or round time) to both clients in the match via Socket.io.

Step 3: Client Countdown Display: On receiving match state updates, if state is COUNTDOWN, display the remaining countdown prominently on the UI (update DOM element). Disable player input controls (movement listener, fire listener).

Step 4: Server Start Round Logic: Create startRound():

Set currentMatchState = MatchState.IN_PROGRESS;.

Start the main round timer: let roundTimeRemaining = ROUND_DURATION_SECONDS; const roundTimerInterval = setInterval(() => { roundTimeRemaining--; broadcastMatchState(); if (roundTimeRemaining === 0) { clearInterval(roundTimerInterval); endRound(); } }, 1000);.

Enable player input processing on the server (allow handling PLAYER_INPUT, PLAYER_FIRE).

Broadcast the new state.

Step 5: Client Start Round: On receiving IN_PROGRESS state, enable input controls. Display round timer, updating based on received roundTimeRemaining.

Step 6: Server End Round Logic: Create endRound():

Set currentMatchState = MatchState.ROUND_OVER;.

Determine winner based on score (see 3.3.3).

Broadcast state and winner info.

Disable player input processing on server.

Potentially start logic for next round or end match.

Why: Provides clear start/stop points for rounds, prevents actions before start, creates time pressure, and synchronizes state across server/clients.

Unit Test: Start match instance. Simulate two players connecting. Verify server state moves to COUNTDOWN, countdown timer starts, state broadcasted. Verify client displays countdown, input disabled. Verify server moves to IN_PROGRESS after countdown, round timer starts. Verify client enables input, displays round timer. Let round timer expire; verify server moves to ROUND_OVER, broadcasts result.

Troubleshooting:

Timers drift: Ensure server uses reliable timing (e.g., setInterval with adjustments or more robust timer loop). Client timers should primarily display data received from server.

Input not disabled/enabled correctly: Check client logic reacting to match state updates. Check server logic enabling/disabling input handlers based on state.

STOP & CHECK: Confirm server manages match states (Countdown, InProgress, RoundOver), controls timers, synchronizes state/time with clients, and client correctly disables/enables input and displays timers based on state.

3.3.2 Score tracking (kills/deaths)

Step 1: Server Player State: Add kills = 0; and deaths = 0; to the server-side player state object for each of the two players in the match. Store player IDs player1Id, player2Id.

Step 2: Server Death Handling Update: In the handlePlayerDeath(victim, killer) function (from 3.2.1):

Increment victim.deaths++;.

If killer exists and killer !== victim: Increment killer.kills++;.

(Optional: Handle suicide case killer === victim - maybe decrement score).

Step 3: State Synchronization: Include kills and deaths for both players in the MessageType.GAME_STATE updates sent periodically or specifically when scores change. const gameStatePayload = { players: { [player1Id]: { health, armor, kills, deaths, state }, [player2Id]: { health, armor, kills, deaths, state } }, matchState, timeRemaining }; socket.emit(MessageType.GAME_STATE, gameStatePayload); (emit to both players).

Step 4: Client Score Display: On receiving GAME_STATE:

Parse the players object.

Update HUD elements (using vanilla JS DOM manipulation) to show Player 1's Kills/Deaths and Player 2's Kills/Deaths. Example: document.getElementById('player1-score').textContent = \${p1Data.kills} / ${p1Data.deaths}`;`.

Step 5: Kill Feed (Optional): When a kill occurs in handlePlayerDeath, emit a separate MessageType.KILL_FEED event: io.to(matchRoomId).emit(MessageType.KILL_FEED, { killerName: killer.name, victimName: victim.name, weaponUsed: killer.weapon });. Client listens for this and displays messages briefly (e.g., appending to a list in the UI).

Why: Tracks the primary objective measure (kills) and consequences (deaths) for the 1v1 duel. Provides players with real-time feedback on performance via HUD/scoreboard.

Unit Test: Server: Simulate Player A killing Player B. Verify playerA.kills increments, playerB.deaths increments in server state. Verify GAME_STATE updates contain the correct scores. Client: Receive the update; verify HUD displays "A: 1 / 0" and "B: 0 / 1". Simulate Player B killing Player A; verify HUD updates to "A: 1 / 1", "B: 1 / 1". Test kill feed if implemented.

Troubleshooting:

Scores incorrect: Check increment logic in handlePlayerDeath. Ensure killer/victim are correctly identified.

HUD not updating: Check client handler for GAME_STATE correctly parses data and updates the relevant DOM elements.

STOP & CHECK: Confirm server accurately tracks kills/deaths for both players, syncs scores via game state updates, and client displays scores correctly on HUD.

3.3.3 Victory/defeat conditions (1v1 duel)

Step 1: Define Win Conditions (Server Config): const KILLS_TO_WIN_ROUND = 5; const ROUNDS_TO_WIN_MATCH = 3;. Store current round wins per player: player1RoundWins = 0; player2RoundWins = 0;.

Step 2: Server Check Conditions: Perform checks within the server loop or after events that change score/state (like a kill):

Check Kill Limit: if (player1.kills >= KILLS_TO_WIN_ROUND || player2.kills >= KILLS_TO_WIN_ROUND) { endRound(); }.

Check Timer Expiration: Done within the round timer logic in 3.3.1 (if (roundTimeRemaining === 0) { endRound(); }).

Step 3: Server Determine Round Winner: In endRound():

Determine roundWinnerId. If timer expired, winner is player with more kills (handle ties?). If kill limit reached, winner is that player.

If roundWinnerId exists, increment their round win count: if (roundWinnerId === player1Id) player1RoundWins++; else if (roundWinnerId === player2Id) player2RoundWins++;.

Broadcast round outcome: io.to(matchRoomId).emit(MessageType.ROUND_OVER, { winnerId: roundWinnerId, scores: { p1Kills, p1Deaths, p2Kills, p2Deaths } });.

Step 4: Server Check Match End: After determining round winner, check match win condition: if (player1RoundWins >= ROUNDS_TO_WIN_MATCH || player2RoundWins >= ROUNDS_TO_WIN_MATCH) { endMatch(); } else { prepareNextRound(); }.

Step 5: Server End Match Logic: Create endMatch():

Set currentMatchState = MatchState.MATCH_OVER;.

Determine matchWinnerId.

Broadcast match outcome: io.to(matchRoomId).emit(MessageType.MATCH_OVER, { winnerId: matchWinnerId, finalRoundWins: { p1Wins, p2Wins } });.

Initiate cleanup/reporting (Phase 4).

Step 6: Server Prepare Next Round: Create prepareNextRound(): Reset player kills/deaths to 0, reset player positions/state via respawnPlayer(), start countdown (3.3.1) for the next round.

Step 7: Client Display Outcome:

On ROUND_OVER, display round winner message.

On MATCH_OVER, display match winner message and potentially final scoreboard. Transition UI (e.g., back to lobby button).

Why: Defines clear goals for the 1v1 duel (win rounds by kills/time, win match by rounds). Manages the flow between rounds and the conclusion of the match.

Unit Test: Set kills=1, rounds=1. Simulate Player A gets 1 kill. Verify endRound called, A declared winner, endMatch called, A declared match winner, state is MATCH_OVER. Test timer expiration: set kills=5, time=10s. Let timer expire with scores A=2, B=1. Verify A wins round. Test multi-round: set rounds=2. Simulate A wins R1, B wins R2, A wins R3. Verify A wins match. Check client displays outcomes correctly.

Troubleshooting:

Incorrect winner: Check win condition logic (>= vs >), tie handling, score comparison.

Match doesn't end/progress: Ensure endRound/endMatch/prepareNextRound are called correctly based on conditions. Check state transitions.

STOP & CHECK: Confirm server correctly evaluates round/match win conditions based on kills, time, or round counts, transitions state, declares winners, progresses rounds, and notifies clients.

Phase 4: Multiplayer Infrastructure

Objective: Enable multiple concurrent 1v1 matches by managing players and isolated game server instances.

4.1 Matchmaking Service

Goal: Pair players efficiently for 1v1 duels, potentially considering skill, and assign them to a game server.

4.1.1 Player rating (ELO) system (Optional but Recommended)

Step 1: Decide if skill-based matchmaking is needed. If yes, choose algorithm (ELO is simplest).

Step 2: Add Database Schema: In persistent DB (e.g., MongoDB), add eloRating (Number, default e.g., 1200) to player profile schema.

Step 3: Implement ELO Calculation (Server): Create a JS function calculateNewRatings(player1Rating, player2Rating, outcome) (outcome = 1 if P1 wins, 0 if P2 wins, 0.5 for draw - unlikely in 1v1 FPS). Use standard ELO formula based on expected score vs actual score.

Step 4: Update Ratings Post-Match (Server): After endMatch() reports winner (from 3.3.3 or 4.2.2), retrieve pre-match ratings, call calculateNewRatings, and update ratings in the database. This logic likely lives outside the game instance process, perhaps in a central API/matchmaking service.

Step 5: Use Rating in Queueing: When player queues (4.1.2), fetch their eloRating to help find opponents.

Why: Creates fairer matches by pairing similarly skilled players, leading to better engagement (optional for initial version).

Unit Test: Create dummy player docs with ratings. Call calculateNewRatings(1200, 1200, 1); verify ratings change appropriately (e.g., 1216, 1184 for K=32). Call with different ratings/outcomes; verify results match ELO principles. Simulate post-match update; check DB for updated ratings.

Troubleshooting:

ELO calculation errors: Double-check the ELO formula implementation.

Ratings not updating: Ensure post-match logic correctly calls the update function and saves to DB.

STOP & CHECK: (If implementing ELO) Confirm system can store, retrieve, and update player ELO ratings based on match outcomes.

4.1.2 Queue management with Redis (or in-memory simple queue)

Step 1: Choose Queue Mechanism:

Simple (In-Memory): For low scale / initial version, use a simple JavaScript array on the main matchmaking server process: const queue = [];. Suitable for maybe tens/hundreds CCU.

Redis: For scalability, use Redis. Add Redis client (npm install redis) to matchmaking service. Connect client.

Step 2: Player Data for Queue: When a player clicks "Find Match", collect necessary info: playerId, eloRating (if using ELO), timeEnteredQueue = Date.now().

Step 3: Add Player to Queue:

In-Memory: queue.push({ playerId, eloRating, timeEnteredQueue });.

Redis: Use a Sorted Set ZADD matchmaking_queue <eloRating> <playerId>. Store timeEnteredQueue in a separate Hash HSET player_queue_data:<playerId> timeEntered <timeEnteredQueue>. (Using ELO as score allows range searching).

Step 4: Remove Player from Queue (Cancel/Disconnect):

In-Memory: Find and remove player object from queue array.

Redis: ZREM matchmaking_queue <playerId>, DEL player_queue_data:<playerId>.

Step 5: Matchmaking Logic (Periodic Check): Run setInterval(findMatch, 3000);.

Inside findMatch:

In-Memory: Iterate through queue. For each player, iterate again to find another player within acceptable ELO range (Math.abs(p1.eloRating - p2.eloRating) < ELO_TOLERANCE). Optionally increase tolerance based on timeEnteredQueue.

Redis: Get a player (e.g., ZRANGE matchmaking_queue 0 0). Query for nearby players using ZRANGEBYSCORE matchmaking_queue (player.elo - tolerance) (player.elo + tolerance). Iterate results to find a compatible match (excluding self). Increase tolerance if needed.

If match found (Player A, Player B):

Remove both players from the queue (using logic from Step 4).

Proceed to assign server (4.1.3).

Why: Provides a mechanism to hold players waiting for a 1v1 match and find suitable opponents based on availability and optionally skill (ELO). Redis offers better performance and scalability for large queues than a simple in-memory array.

Unit Test: Simulate players entering queue with different ELOs. Verify they are added (check queue array length or Redis ZCARD/HGET). Run findMatch. Verify compatible players are paired and removed (check queue again). Test cancellation logic. Test increasing ELO tolerance over time if implemented.

Troubleshooting:

No matches found: Check ELO tolerance, queue population. Ensure iteration/query logic is correct.

Incorrect matches: Verify ELO range checking logic.

Redis issues: Check Redis connection, command syntax, key names.

STOP & CHECK: Confirm players can queue, be removed, and the system periodically attempts to find and pair compatible 1v1 opponents based on chosen criteria (availability +/- ELO).

4.1.3 Server allocation and redirection

Step 1: Game Server Registry: Need a way for idle game server instances (from 4.2) to announce their availability.

Simple (In-Memory): Maintain an array availableServers = [{ id, ip, port }, ...]; on the matchmaking service. Game instances call an API endpoint on the matchmaker to register/deregister. Not scalable/robust.

Redis: Use a Redis Set SADD available_servers <serverId>. Store server details (IP/port) in a Hash HSET server_details:<serverId> ip <ip> port <port>. Game instances add themselves on startup, remove themselves on shutdown/allocation.

Step 2: Allocate Server: When findMatch successfully pairs Player A and Player B (from 4.1.2):

Get an available server ID:

In-Memory: const serverInfo = availableServers.pop();. Check if empty.

Redis: const serverId = SPOP available_servers;. Check if null.

If a server is found:

Retrieve server details (IP/port) if needed (e.g., from Redis Hash HGETALL server_details:<serverId>).

Mark server as busy (e.g., add to a busy_servers Redis Set, or remove details hash).

Step 3: Notify Players: Send a message via Socket.io (presumably players are still connected to the matchmaking service) to both Player A and Player B, containing the allocated game server's connection details (IP, port, maybe a temporary match token). socketA.emit(MessageType.MATCH_FOUND, { serverIp, serverPort, matchId }); socketB.emit(MessageType.MATCH_FOUND, { serverIp, serverPort, matchId });.

Step 4: Client Connection Logic: On receiving MATCH_FOUND, the client should:

Disconnect from the matchmaking Socket.io server (optional, depends on architecture).

Initiate a new Socket.io connection to the specified serverIp and serverPort of the dedicated game instance.

Potentially send the matchId or token upon connecting to the game instance for identification.

Why: Connects the abstract concept of a "match found" to a concrete, running game server instance capable of hosting the 1v1 duel. Directs the matched clients to the correct place to start playing.

Unit Test: Simulate idle game servers registering in the registry (populate availableServers array or Redis Set/Hash). Run findMatch. Verify it allocates a server, removes it from available pool (check array/Redis), and gets connection details. Simulate clients receiving MATCH_FOUND message with IP/port. Verify clients attempt connection to the new address (log connection attempts).

Troubleshooting:

No available servers: Ensure game instances (Phase 4.2) are running and correctly registering themselves. Check registry state.

Clients don't connect to game server: Verify correct IP/port sent in MATCH_FOUND. Check firewall rules. Ensure game instance is listening correctly. Check client connection logic.

STOP & CHECK: Confirm the matchmaking service allocates an available game server instance from a registry, marks it busy, and sends its connection details to the matched clients, enabling them to connect to the dedicated instance.

4.2 Game Instance Isolation

Goal: Run each 1v1 duel in its own isolated Node.js process for stability and resource management.

4.2.1 Dedicated Node.js process per match

Step 1: Package Game Server Logic: Ensure all server-side code required to run one 1v1 match (Socket.io server setup on a specific port, Rapier world initialization, map loading, player handling for 2 players, game state loop, combat logic, round flow from Phases 1-3) is encapsulated within a single runnable Node.js script (e.g., server/src/gameInstance.js).

Step 2: Parameterize Startup: Modify gameInstance.js to accept necessary startup parameters, e.g., via command-line arguments (process.argv) or environment variables (process.env). Essential parameters: PORT to listen on, unique MATCH_ID. Optional: PLAYER1_ID, PLAYER2_ID (or expect them to connect and provide ID).

Step 3: Spawning Mechanism: Implement a separate "spawner" service or logic (could be part of matchmaking or a standalone manager). This service will use Node.js child_process.fork() (or spawn) to launch gameInstance.js as a new process when a server is needed. import { fork } from 'child_process'; const gameProcess = fork('path/to/server/src/gameInstance.js', ['--port', portNumber, '--matchId', newMatchId]);.

Step 4: Port Allocation: The spawner needs to allocate a unique port number for each new game instance it launches. Maintain a pool of available ports or use OS-assigned ports.

Why: Isolates each match's execution environment. A crash in one duel doesn't affect others. Allows CPU/memory resources to be tied to specific matches. Facilitates independent scaling of match capacity.

Unit Test: Manually run node server/src/gameInstance.js --port 3001 --matchId test1. Verify it starts, logs listening on port 3001. Run again with port 3002. Verify both run concurrently. Use the spawner logic to launch an instance; check ps or Task Manager for the new Node.js process. Verify the process receives and uses the arguments (log them on startup).

Troubleshooting:

Process doesn't start: Check path in fork(). Check for syntax errors in gameInstance.js.

Port conflicts: Ensure unique ports are allocated and passed correctly.

Arguments not received: Check process.argv parsing logic or environment variable access.

STOP & CHECK: Confirm the game server logic can run as an independent Node.js process launched via child_process, accepting parameters like port and match ID.

4.2.2 Process lifecycle management

Step 1: Game Instance Startup Sequence: Inside gameInstance.js, after initialization (Socket.io listening, Rapier world ready):

Register with the central registry (e.g., add self to available_servers Redis Set from 4.1.3, including allocated port and server ID/match ID). Mark status as 'available'.

Wait for the two expected players to connect (check IDs upon connection if provided at startup, or use a simple counter).

Step 2: Game Instance Active State: Once both players connect:

Update status in registry to 'in_progress'.

Start the match flow (countdown -> in_progress rounds -> etc. from Phase 3.3).

Step 3: Game Instance Shutdown Sequence: When endMatch() (from 3.3.3) is called:

Report match results: Send necessary data (winner, scores, ELO info if used) to a persistent storage API endpoint or service.

Gracefully disconnect clients: socket.disconnect(true);.

Clean up resources: Clear intervals (clearInterval), close external connections (DB, Redis if used directly by instance).

Deregister from registry: Remove self from available_servers or busy_servers Redis sets. Delete server details hash.

Exit cleanly: process.exit(0);.

Step 4: Spawner Monitoring: The process that fork()ed the game instance should monitor the child process's exit event: gameProcess.on('exit', (code) => { console.log(\Game instance ${newMatchId} exited with code ${code}`); /* Handle cleanup/logging */ });`.

Why: Defines the complete operational flow of a game instance from becoming available, running a match, reporting results, cleaning up, and terminating, allowing the spawner/orchestrator to manage the pool effectively.

Unit Test: Launch an instance via spawner. Verify it registers in Redis as 'available'. Simulate players connecting; verify status changes to 'in_progress'. Simulate match end; verify result reporting is attempted (log/mock endpoint). Verify clients disconnected. Verify instance deregisters from Redis. Monitor spawner logs for the 'exit' event with code 0.

Troubleshooting:

Instance not registering/deregistering: Check registry connection/logic within gameInstance.js.

Results not reported: Check reporting logic and endpoint availability before exit.

Process doesn't exit cleanly: Look for hanging resources, unclosed intervals/timeouts, or errors during shutdown. Check exit code in spawner log.

STOP & CHECK: Confirm the game instance process manages its lifecycle: registers availability, runs the match, reports results, deregisters, and exits cleanly upon match completion.

4.2.3 Crash recovery system

Step 1: Implement Health Checks (Optional but Good): The gameInstance.js could expose a simple HTTP endpoint (e.g., using built-in http module on a separate port) that the spawner/orchestrator can periodically ping to check liveness.

Step 2: Spawner Crash Detection: The parent spawner process relies on the exit event from child_process. A non-zero code in gameProcess.on('exit', (code) => { ... }); indicates a crash (or non-clean exit).

Step 3: Crash Handling Logic (Spawner): Inside the exit event handler:

Check if (code !== 0).

Log the crash: console.error(\Game instance ${matchId} CRASHED with code ${code}`);`. Include player IDs if known by the spawner.

Clean up Registry: Forcefully remove the serverId associated with the crashed process from any Redis sets (available_servers, busy_servers) and delete its details hash. This prevents matchmaking from assigning players to a dead server.

Mark Match as Aborted (Optional): Notify a central service or update DB to mark the specific matchId as inconclusive due to server error.

Decide on Replacement: The spawner might decide to immediately launch a new idle instance to maintain a target pool size of available servers.

Step 4: Internal Error Handling (Game Instance): Add try...catch blocks around major sections in gameInstance.js (e.g., physics step, message handlers) and implement process.on('uncaughtException', (err) => { console.error('UNCAUGHT EXCEPTION:', err); /* Log details */ process.exit(1); }); to log errors before crashing, aiding debugging.

Why: Ensures that failed game instances don't linger in the system, cleans up their state, logs the failure, and allows the system to maintain a healthy pool of available servers by replacing crashed ones. Makes the overall system resilient to individual instance failures.

Unit Test: Launch an instance via spawner. Forcefully kill the child process (gameProcess.kill() or kill -9 its PID). Verify the spawner's exit handler detects the non-zero code (or signal). Verify crash is logged. Check Redis registry; verify the server ID is removed. Introduce an throw new Error('Test Crash') inside gameInstance.js; launch it; verify it crashes, the uncaughtException handler logs the error, it exits with code 1, and the spawner handles the crash event.

Troubleshooting:

Spawner doesn't detect crash: Ensure the exit event listener is correctly attached to the child process object returned by fork().

Registry cleanup fails: Check Redis connection/logic in the spawner's crash handler. Ensure serverId is correctly identified.

STOP & CHECK: Confirm the spawner system detects game instance crashes (non-zero exit codes), logs them, reliably cleans up their state in the central registry, and potentially spawns replacements.

4.3 State Persistence

Goal: Store essential long-term data like player profiles, ELO, and match history.

4.3.1 Redis session storage (Optional - If complex session needed beyond queue)

Step 1: Evaluate Need: Determine if data beyond matchmaking queue status needs fast, temporary storage during a user's online session (e.g., selected character skin before a match starts, party information if parties exist, temporary preferences). If not strictly needed, skip this for simplicity.

Step 2: If needed, use Redis Hashes: When player logs into the main backend/API: redisClient.hSet(\session:
playerId,ˋstatus:′online′,lastSeen:Date.now(),/∗othertempdata∗/);redisClient.expire(sˋession:
playerId
,
ˋ
	​

status:
′
online
′
,lastSeen:Date.now(),/∗othertempdata∗/);redisClient.expire(
s
ˋ
ession:
{playerId}`, 3600); // e.g., 1 hour TTL`.

Step 3: Update/Read Session: Other services (matchmaking, API) can read/update this hash: const status = await redisClient.hGet(\session:${playerId}`, 'status');. UpdatelastSeen` periodically or on activity.

Step 4: Use TTLs: Rely on Redis TTL expiration to eventually clean up data for disconnected users.

Why: Provides fast, shared access to temporary user session state, potentially across multiple backend microservices, without hitting the persistent DB constantly.

Unit Test: Simulate player login; check Redis hash created with TTL using redis-cli (HGETALL, TTL). Simulate activity; update hash; check changes. Wait for TTL; verify hash expires.

Troubleshooting:

Session data incorrect/missing: Check Redis keys, HSET/HGET logic, TTL values.

Stale data: Ensure TTLs are appropriate and lastSeen is updated.

STOP & CHECK: (If implemented) Confirm Redis is used for storing temporary, shared user session data with appropriate TTLs.

4.3.2 MongoDB match history

Step 1: Add DB Driver: In the service responsible for receiving match results (e.g., matchmaking service or a dedicated results API), npm install mongodb. Connect the MongoDB client.

Step 2: Define Schema (Conceptual): Plan the structure for the matches collection document: { _id: ObjectId, matchId: String, // Unique ID generated for the match gameMode: '1v1_duel', mapName: String, startTime: Date, endTime: Date, durationSeconds: Number, players: [ { playerId: String, team: 'A'/'B', preMatchElo: Number, postMatchElo: Number, kills: Number, deaths: Number }, { playerId: String, team: 'A'/'B', preMatchElo: Number, postMatchElo: Number, kills: Number, deaths: Number } ], roundWins: { playerAId: Number, playerBId: Number }, winnerPlayerId: String | null, // Null for draw/abort abortReason: String | null // If match crashed }.

Step 3: Implement Result Handling: When the match result reporting logic (e.g., called before process.exit(0) in gameInstance.js, sending data to the results service) provides the final match data:

Format the data according to the schema.

Insert into MongoDB: const matchesCollection = db.collection('matches'); await matchesCollection.insertOne(matchResultDocument);.

Step 4: Create Indexes: Ensure indexes exist for efficient querying: matchesCollection.createIndex({ "players.playerId": 1 }); matchesCollection.createIndex({ startTime: -1 }); matchesCollection.createIndex({ matchId: 1 }, { unique: true });.

Why: Persistently stores detailed records of completed matches for player history, analysis, and potentially leaderboards or dispute resolution. MongoDB's flexibility suits potentially evolving match data.

Unit Test: Simulate a completed match report. Verify the results service inserts a document into the matches collection. Query MongoDB directly (mongosh or Compass) to inspect the document; check all fields and values. Query by players.playerId; verify the match shows up. Check indexes exist.

Troubleshooting:

Data not saving: Check MongoDB connection. Check insertion logic and error handling (try...catch). Verify data format matches schema expectations (though Mongo is flexible).

Slow queries: Analyze query patterns; ensure necessary indexes are created using explain().

STOP & CHECK: Confirm detailed match results are reliably saved to a persistent MongoDB collection with an appropriate schema and indexes.

4.3.3 Leaderboard aggregation (ELO based)

Step 1: Leaderboard Cache: Use Redis Sorted Sets for efficient ranking. Key: leaderboard:elo. Score: ELO rating. Member: Player ID.

Step 2: Update Strategy:

On ELO Update: Simpler for real-time feel. When the ELO calculation service (from 4.1.1) updates a player's ELO in the persistent DB (MongoDB), also update their score in the Redis sorted set: redisClient.zAdd('leaderboard:elo', { score: newEloRating, value: playerId });.

(Alternative) Periodic Job: Create a background job (setInterval or cron) that periodically queries the player profiles from MongoDB, sorts by ELO, and overwrites the Redis sorted set with the top N players. Less real-time but potentially simpler consistency.

Step 3: API Endpoint: Create a backend API endpoint /leaderboard/elo.

It queries Redis: const topPlayers = await redisClient.zRangeWithScores('leaderboard:elo', 0, 99, { REV: true }); // Get top 100, highest score first.

Optionally, fetch player names from another Redis hash or the persistent DB based on the retrieved IDs.

Return the ranked list (PlayerID, ELO, Rank, Name).

Step 4: Client Display: Client calls the /leaderboard/elo endpoint and displays the ranked list in the UI.

Why: Provides fast access to player rankings without querying and sorting the entire player database on every request. Redis sorted sets are optimized for this use case.

Unit Test: Update several players' ELOs via the update mechanism (direct or periodic job). Check Redis ZRANGE leaderboard:elo 0 -1 WITHSCORES REV to verify players are present and ranked correctly by ELO. Call the API endpoint; verify it returns the expected ranked data read from Redis.

Troubleshooting:

Leaderboard incorrect/stale: Check Redis update logic (is ZADD being called correctly with right score/value?). If using periodic job, check job frequency and query/sorting logic.

API slow: Ensure API reads only from Redis cache, not the main DB for ranking. Check Redis performance.

STOP & CHECK: Confirm player ELO ratings are used to maintain a ranked leaderboard, efficiently stored (likely in Redis), and retrievable via an API for client display.

Phase 5: Optimization & Scaling

Objective: Enhance performance and efficiency to handle potentially large numbers of concurrent 1v1 duels and prepare for growth.

5.1 Network Optimization
Goal: Reduce bandwidth usage and minimize latency impact for Rapier state synchronization.

5.1.1 Delta state compression (Rapier State)

Step 1: Identify High-Frequency State: The most frequently changing data sent in GAME_STATE is likely player position (translation) and rotation (rotation) from Rapier bodies. Health/armor/score change less often.

Step 2: Track Acknowledged State (Server): For each connected client, store the last game state snapshot successfully sent or acknowledged by them (requires ack mechanism or assuming reliable transport like Socket.io default). Store at least the last sent position/rotation/velocity for each player relevant to that client.

Step 3: Compare States (Server): When preparing the next GAME_STATE update for a client:

Get the current authoritative Rapier state (pos, rot, vel) for relevant players (based on interest management - 5.2.2).

Compare current pos with last sent pos. Compare rot with last sent rot. Use a small epsilon for float comparisons (Math.abs(current.x - last.x) > EPSILON).

Step 4: Build Delta Packet: Create a payload containing only the changed components. Use flags or a compact structure: { tick: serverTick, updates: [ { playerId: 'p1', pos?: {x,y,z}, rot?: {x,y,z,w}, vel?: {x,y,z}, health?: number /* only if changed */ }, ... ] }. The ? indicates optional fields. Client needs logic to merge these partial updates.

Step 5: Consider Binary Encoding: For further optimization, use a binary serialization format (like msgpack or Protocol Buffers, requires npm install msgpackr etc.) instead of JSON for these frequent state updates. Define a strict binary schema.

Why: Significantly reduces the size of game state updates by omitting unchanged data, saving bandwidth and potentially reducing processing load on both ends. Crucial for scaling player counts or increasing update frequency.

Unit Test: Log the size of the JSON GAME_STATE packet before delta compression. Implement delta logic. Log the size of the delta packet when only position changes, vs only health changes, vs multiple changes. Verify client correctly reconstructs the full state by applying deltas onto its previous state. Measure network traffic difference under simulated gameplay. Test binary encoding size reduction.

Troubleshooting:

Client state incorrect: Ensure delta application logic on client correctly merges partial updates. Check for missed packets if using unreliable transport (though Socket.io is reliable by default). Ensure baseline state is established correctly. Check float comparison epsilon.

Binary encoding errors: Verify schema definitions match on client/server. Check encoding/decoding library usage.

STOP & CHECK: Confirm server sends delta-compressed Rapier/game state updates, client correctly applies them, and this results in significantly reduced network bandwidth usage.

5.1.2 Packet prioritization (if needed beyond Socket.io reliability)

Step 1: Evaluate Need: Socket.io's default TCP-based transport is reliable and ordered. Explicit prioritization is often less critical than with UDP unless trying advanced techniques or facing extreme congestion where ordering of different streams matters. For 1v1, standard Socket.io might suffice initially. Re-evaluate under heavy load testing (7.3.1).

Step 2: If Needed (e.g., Custom Transport/WebRTC): Categorize Socket.io events by importance: Critical (PLAYER_INPUT, critical GAME_STATE deltas like position/rotation), Less Critical (KILL_FEED, ROUND_OVER summaries, chat).

Step 3: Implement Sending Logic (Server): If facing issues where large non-critical messages block critical ones despite Socket.io's buffering, consider:

Sending less critical data less frequently or aggregating it.

Breaking large state updates into smaller chunks (if possible).

Advanced: Explore alternative transports like WebRTC data channels which offer reliable/unreliable modes, allowing explicit separation. (Adds significant complexity).

Why: Ensures the most time-sensitive gameplay data (inputs, positions) has the best chance of arriving promptly, even if the network link is temporarily saturated. Less critical for reliable streams like default Socket.io unless dealing with head-of-line blocking internally.

Unit Test: (Hard to test reliably without custom transport or specific congestion scenarios). Simulate sending large amounts of low-priority data alongside high-priority data. Measure end-to-end latency for high-priority messages. If latency increases significantly due to low-priority traffic, investigate prioritization needs.

Troubleshooting:

High latency under load: Profile server event loop, check Socket.io buffer sizes, consider message aggregation or delta compression first. Explicit prioritization is often a later optimization if simpler methods fail.

STOP & CHECK: Confirm message types are categorized by importance and, if necessary based on load testing and transport choice, mechanisms are in place to prioritize critical data delivery.

5.1.3 Bandwidth throttling (Server Control)

Step 1: Monitor Outgoing Bandwidth (Server): Add instrumentation within the server's Socket.io sending logic (or potentially at a higher proxy level) to track bytes sent per client over time windows (e.g., per second).

Step 2: Define Limits: Set a configurable max bytes/second per client (e.g., MAX_KBPS_PER_CLIENT = 20).

Step 3: Implement Rate Limiter: Before sending a packet via socket.emit():

Check if adding the packet's size to the current window's byte count exceeds MAX_KBPS_PER_CLIENT * windowDurationSeconds.

If limit exceeded, queue the packet (potentially dropping low-priority ones if queue fills) or delay sending until the next window. A token bucket algorithm is suitable here.

Why: Controls server egress bandwidth costs. Prevents single clients from consuming disproportionate resources. Can help maintain stability under certain types of load or network conditions.

Unit Test: Configure a limit (e.g., 10 KBps). Simulate server sending high volume of data (e.g., frequent large state updates) to one client. Use server-side network monitoring tools (iftop, nethogs or cloud provider metrics) to measure actual outgoing rate to that client's IP. Verify rate stays close to the configured limit. Observe queuing/dropping behavior if implemented.

Troubleshooting:

Limiter inaccurate: Check byte counting logic. Check time window calculation. Ensure limit applies correctly before socket.emit.

Gameplay negatively impacted: If throttling kicks in too aggressively, increase limit or optimize packet sizes further (delta compression, binary format).

STOP & CHECK: Confirm server can enforce configurable outgoing bandwidth limits per client.

5.2 Spatial Partitioning (Server Optimization)

Goal: Reduce server CPU load by optimizing proximity-based queries within the Rapier world (less critical for 1v1, but good practice for potential expansion).

5.2.1 Rapier's Built-in Broadphase/Query Pipeline

Step 1: Understand Rapier's Internals: Recognize that Rapier itself already uses sophisticated spatial partitioning structures (like a DBVT - Dynamic Bounding Volume Tree) internally as part of its broadphase collision detection pipeline. Direct manual implementation (like an Octree on top of Rapier) is usually unnecessary for physics queries.

Step 2: Utilize Rapier Queries: Leverage Rapier's efficient query pipeline for proximity checks when needed (e.g., Area of Effect abilities, custom interest management if expanding beyond 1v1).

world.collidersWithaabbIntersectingAabb(aabb, callback): Find colliders intersecting a box.

world.intersectionsWithShape(shapePos, shapeRot, shape, filter, callback): Find colliders intersecting a given shape (sphere, capsule, etc.).

Why: Avoids reinventing the wheel. Rapier's internal structures are highly optimized for the physics simulation's needs. Using its query API leverages these optimizations directly. Manual structures are more relevant if managing non-physics entities or very specific query types not covered by Rapier's API.

Unit Test: Create several Rapier colliders in the server world. Define an AABB or a shape (e.g., new RAPIER.Ball(radius)). Call the relevant Rapier query function (collidersWithaabbIntersectingAabb or intersectionsWithShape). Verify the callback receives only the colliders that spatially overlap with the query area/shape according to Rapier's simulation state. Test performance with many colliders.

Troubleshooting:

Query returns unexpected results: Check query parameters (AABB definition, shape position/rotation). Check filter flags/groups used in the query. Verify collider positions in the world.

STOP & CHECK: Confirm understanding and ability to use Rapier's built-in query pipeline (collidersWithaabbIntersectingAabb, intersectionsWithShape) for efficient spatial queries when needed.

5.2.2 Interest management system (Simplified for 1v1)

Step 1: Evaluate Need for 1v1: In a strict 1v1 duel on a reasonably sized map, both players are almost always relevant to each other. A complex Area of Interest (AoI) system might be overkill. The primary benefit comes from not broadcasting state to unrelated matches.

Step 2: Basic Implementation (Per-Match Broadcast): Since each match runs in an isolated process (4.2.1) hosting only two players, simply broadcasting the GAME_STATE (containing state for both players) to both connected sockets within that specific match instance effectively achieves basic interest management. Player A receives updates about Player B, and vice-versa, because they are the only relevant entities in that world instance.

Step 3: Future Expansion: If expanding to more players per instance or larger worlds, implement explicit AoI:

For each player, define an AoI radius.

Before sending GAME_STATE to Player A, use Rapier queries (intersectionsWithShape with a sphere shape around Player A) to find entities (Player B, objects C, D) within the radius.

Only include state for entities B, C, D in the packet sent to A.

Why: For 1v1, isolation provides implicit interest management. Explicit AoI using Rapier queries becomes necessary only if player/entity counts per instance increase significantly, saving bandwidth and client processing by omitting irrelevant updates.

Unit Test: (For 1v1): Verify that GAME_STATE updates sent from a game instance only contain data for the two players in that match and are only sent to those two sockets. (For future AoI): Simulate 3 players; use Rapier query around Player A; verify only nearby Player B's state is included in A's update packet, while distant Player C is omitted.

Troubleshooting:

(Future AoI) Player disappears/stutters at range: AoI radius might be too small. Query logic might be incorrect. Ensure state is sent reliably when entity enters AoI.

STOP & CHECK: Confirm that for the 1v1 setup, game state updates are correctly scoped to the two players within the match instance. Understand how Rapier queries would be used for explicit AoI if needed later.

5.2.3 LOD model switching (Client-Side Three.js)

Step 1: Prepare Assets: Obtain/create multiple versions of character/weapon models (High, Medium, Low poly count). Ensure origins/rigging are consistent. Export as separate GLBs or structure them within one GLB if loader supports selecting specific meshes.

Step 2: Load LODs (Client): Load all LOD variations for a character/weapon when needed. Store references to the different THREE.Object3D meshes.

Step 3: Implement THREE.LOD Object (Client): Use Three.js's built-in LOD helper object. const lod = new THREE.LOD(); scene.add(lod);.

Step 4: Add Levels to LOD Object: Add each loaded mesh variation with a distance threshold: lod.addLevel(highPolyMesh, 20); lod.addLevel(mediumPolyMesh, 50); lod.addLevel(lowPolyMesh, 100);. Distances are in world units from the camera. The LOD object automatically handles switching visibility based on camera distance.

Step 5: Update LOD Position: In the render loop, update the LOD object's position and rotation to match the player's interpolated position (for remote players) or predicted position (for local player). lod.position.copy(interpolatedPos); lod.quaternion.copy(interpolatedRot);. Also, crucially, call lod.update(camera); in the render loop – this tells the LOD object to check distances and switch levels.

Step 6: Animation Synchronization: Ensure the AnimationMixer targets the LOD object or that animation state is correctly applied to whichever mesh (high/medium/low) is currently visible within the LOD structure. This might require careful handling of the mixer's root object or applying animations to all levels simultaneously.

Why: Improves client rendering performance (FPS) by reducing polygon count for distant objects, allowing more complex scenes or smoother play on lower-end hardware. THREE.LOD simplifies the distance checking and visibility switching logic.

Unit Test: Load a character using THREE.LOD with visible high/medium/low meshes. Move the camera far away; verify only low-poly visible (check wireframe/poly count). Move closer; verify medium then high poly versions switch in at the correct distance thresholds defined in addLevel. Ensure animations still play correctly on the currently visible LOD mesh.

Troubleshooting:

LODs not switching: Ensure lod.update(camera) is called every frame. Check distance thresholds. Verify camera and LOD object positions are updating correctly in world space.

Visual "pop" during switch: Adjust distance thresholds for smoother transition points. Ensure model origins/scales are identical across LODs. Consider cross-fading techniques if popping is severe (more complex than base THREE.LOD).

Animations broken: Check how the AnimationMixer is connected relative to the LOD object and its children meshes.

STOP & CHECK: Confirm client uses THREE.LOD object to automatically switch model detail based on camera distance, optimizing rendering performance.

5.3 Horizontal Scaling (Node.js/Kubernetes)

Goal: Distribute the 1v1 duel load across multiple machines/containers using Node.js game instances managed potentially by Kubernetes.

5.3.1 Kubernetes cluster configuration (Optional but Recommended for Scale)

Step 1: Containerize Game Instance: Create a Dockerfile for the Node.js game instance (server/src/gameInstance.js and its dependencies). Ensure it installs production dependencies, copies necessary code, and defines the CMD or ENTRYPOINT to run the instance (e.g., CMD ["node", "server/src/gameInstance.js"]). Build the Docker image. Push to a container registry (Docker Hub, GCR, ECR, ACR).

Step 2: Define Kubernetes Deployment/StatefulSet: Create a game-server.yaml file.

Define a Deployment (if instances are mostly stateless beyond a single match) or StatefulSet (if stable network identifiers per instance are needed).

Specify the Docker image location.

Request resources (spec.template.spec.containers[].resources: { requests: { cpu: "250m", memory: "256Mi" }, limits: { cpu: "500m", memory: "512Mi" } } - adjust based on profiling).

Define how parameters are passed (e.g., environment variables for allocated port, match ID if needed at pod level, registry info). Ports might be managed by Kubernetes networking instead of passed manually if using Services.

Step 3: Configure Networking: Decide how clients connect:

NodePort/LoadBalancer Service: Create a Kubernetes Service of type LoadBalancer or NodePort for each game server pod. Matchmaking assigns the external IP/Port of a specific service to clients. Can be complex/costly at scale.

HostPort: Expose the container port directly on the node's IP. Simpler but requires careful port management and less flexible scaling.

Ingress/Gateway (Advanced): Use a dedicated WebSocket-aware ingress controller or API Gateway that can route connections to the correct backend pod based on path or headers (e.g., containing match ID). More complex setup but better network management. (Start simple, maybe HostPort or NodePort initially).

Step 4: Deploy Other Services: Create similar Dockerfiles and Kubernetes YAML definitions for the matchmaking service, API backend, result reporting service, etc. Define Services for internal communication between them.

Step 5: Apply Configuration: Use kubectl apply -f <yaml-file> to deploy services to the chosen Kubernetes cluster (GKE, EKS, AKS, self-hosted).

Why: Kubernetes automates deployment, scaling, failure handling, and networking of containerized applications like the Node.js game instances, making it feasible to manage hundreds or thousands of concurrent match servers reliably.

Unit Test: Build Docker image. Apply game-server.yaml to cluster. Check pod status: kubectl get pods. Verify pod is 'Running'. Check logs: kubectl logs <pod-name>. Verify game instance started correctly inside the container. Manually scale deployment: kubectl scale deployment game-server --replicas=3; verify more pods start. Test chosen networking approach: Can you connect to the game instance's exposed port/service from outside the cluster?

Troubleshooting:

Pod CrashLoopBackOff: Check pod logs (kubectl logs) for errors within the container (Node.js errors). Check Dockerfile/image. Check resource limits vs actual usage.

Network connection refused: Verify Kubernetes Service/Ingress configuration. Check firewall rules (cluster internal and external). Ensure game instance listens on correct port inside container (0.0.0.0 usually).

STOP & CHECK: Confirm the Node.js game instance application is containerized via Docker and can be deployed, replicated, and accessed via networking within a Kubernetes cluster.

5.3.2 Auto-scaling triggers (CPU/Available Instances)

Step 1: Define Scaling Metric: Choose primary metric for scaling the pool of idle game server instances. Good options:

Available Server Pool Size (Custom Metric): Monitor the size of the available_servers set in Redis (from 4.1.3). Aim to maintain a buffer (e.g., always keep 10-20 idle servers available).

CPU Utilization (HPA): Scale based on average CPU across running game server pods. Less direct control over idle pool size.

(Start with custom metric based on available pool size).

Step 2: Implement Custom Scaling Logic (if using custom metric):

Create a small, separate "Autoscaler" service (can be a simple Node.js script running as a Kubernetes CronJob or Deployment).

This service periodically queries Redis: const availableCount = await redisClient.sCard('available_servers');.

It gets the current replica count of the game server Deployment: kubectl get deployment game-server -o jsonpath='{.spec.replicas}' (needs Kubernetes API client library or kubectl access).

If availableCount < MIN_IDLE_BUFFER, calculate needed replicas and scale up: kubectl scale deployment game-server --replicas=<new_higher_count>.

If availableCount > MAX_IDLE_BUFFER, calculate new count and scale down: kubectl scale deployment game-server --replicas=<new_lower_count>. Be careful not to scale down below the number of currently busy servers.

Step 3: Configure HPA (if using CPU/Memory): Create HorizontalPodAutoscaler YAML targeting the game server Deployment. Specify targetCPUUtilizationPercentage and min/max replicas. Requires Kubernetes Metrics Server to be installed.

Step 4: Configure Cluster Autoscaler: In the cloud provider, enable and configure the cluster autoscaler to add/remove VM nodes to/from the Kubernetes cluster itself based on pending pod resource requests. This ensures Kubernetes has nodes to place scaled-up pods onto.

Why: Automatically adjusts the number of running game server processes to meet player demand, keeping matchmaking times low during peaks and saving costs during lulls. Custom metrics often provide better game-specific scaling than generic CPU/memory.

Unit Test: (Custom Logic): Manually adjust the count in available_servers Redis set below MIN_IDLE_BUFFER. Verify the Autoscaler service detects this and scales up the Deployment replicas (check kubectl get deployment). Manually add many servers to the set (above MAX_IDLE_BUFFER); verify Autoscaler scales down replicas. (HPA): Apply CPU load to existing pods; verify HPA increases replica count (kubectl get hpa). Test cluster autoscaler by scaling Deployment replicas beyond current node capacity; verify new nodes are added to cluster.

Troubleshooting:

Scaling not happening: Check Autoscaler service logs/logic. Check Redis queries. Check permissions to use kubectl or Kubernetes API. Check HPA configuration and metrics server status. Check cluster autoscaler logs/configuration.

Scaling too slow/fast: Adjust buffer sizes (MIN/MAX_IDLE), check frequency, HPA thresholds/cooldowns.

STOP & CHECK: Confirm the system automatically scales the number of game server instances (Kubernetes pods) up/down based on available instance count or resource utilization, ensuring capacity matches demand.

5.3.3 Global server region deployment (Advanced)

Step 1: Plan Regions: Identify target geographic regions (e.g., us-east1, europe-west1, asia-northeast1).

Step 2: Deploy Stacks per Region: Set up infrastructure (Kubernetes clusters, databases, Redis) and deploy the entire application stack (matchmaker, game servers, API) independently in each target region. Use infrastructure-as-code tools (Terraform, Pulumi) for consistency.

Step 3: Geo-DNS / Latency Routing: Configure a global DNS service (e.g., AWS Route 53, Google Cloud DNS) for the main entry point (e.g., matchmaking API URL). Use latency-based routing records that direct players to the regional endpoint with the lowest latency from their location. Health checks should be configured to route away from unhealthy regions.

Step 4: Regional Matchmaking: Ensure the matchmaking service queried by a player (after being routed by DNS) only interacts with the Redis queue and game server registry for that specific region. Matches should be formed within the region.

Step 5: Global Data Synchronization (If Needed): For data needed across regions (e.g., player profiles, ELO, global leaderboards):

Use globally replicated databases (e.g., Cosmos DB, Spanner, MongoDB Atlas Global Clusters).

Or implement asynchronous replication between regional databases. Requires careful handling of consistency. (Keep it simple initially - maybe player profiles are global but matchmaking/gameplay is strictly regional).

Why: Provides lowest possible latency for players by connecting them to physically closer servers. Increases fault tolerance (region outage doesn't affect others). Allows scaling capacity geographically.

Unit Test: Deploy to two regions. Use tools/VPNs to simulate connecting from locations near each region. Verify DNS routes to the correct regional endpoint. Verify matchmaking occurs within the region and assigns game servers from that region's pool. Test latency difference connecting to near vs far region. Simulate regional outage (take down services in one region); verify DNS health checks route traffic to the healthy region. Check global data consistency if implemented.

Troubleshooting:

Incorrect routing: Check DNS configuration (latency records, health checks).

Cross-region matching: Ensure matchmaking logic is strictly scoped to its own region's queue/registry.

Global data consistency issues: Complex problem. Review database replication configuration, consistency models, conflict resolution strategies if using multi-region writes.

STOP & CHECK: Confirm the application stack can be deployed and operate independently in multiple geographic regions, with players routed to the lowest-latency region for matchmaking and gameplay.

Okay, let's continue with the extremely detailed breakdown for Phases 6 and 7, tailored for the Three.js (Rendering) + Rapier (Physics) + Socket.io (Networking) + Plain JavaScript stack for a 1v1 FPS duel, guiding the AI step-by-step with tips and recommendations.

Phase 6: Security & Anti-Cheat

Objective: Maintain a fair competitive environment by deterring, detecting, and preventing cheating, acknowledging the inherent limitations of client-side JavaScript security but maximizing server-side authority.

6.1 Client Hardening

Goal: Make it more difficult (but not impossible) for malicious actors to analyze, modify, or inject code into the client-side JavaScript application. Crucial Recommendation: Understand that client-side JS offers limited security against determined attackers. The primary defense line must be server-side validation (6.2). Client hardening is about raising the bar for casual cheating.

6.1.1 Code obfuscation

Step 1: Select Obfuscation Tool: Choose a reputable JavaScript obfuscator tool that integrates with your build process (if using one like Vite, Webpack, or Rollup) or can be run standalone on the final JS bundle(s). Examples: javascript-obfuscator. Install it as a dev dependency.

Step 2: Configure Obfuscation: In your build configuration (e.g., vite.config.js, webpack.config.js) or a separate script:

Enable obfuscation only for production builds (NODE_ENV === 'production'). Keep development builds unobfuscated for debugging.

Configure options carefully:

renameGlobals: Renames top-level variables/functions.

controlFlowFlattening: Makes code execution path harder to follow. (Can impact performance, test!).

stringArray: Moves strings into an encrypted array, referenced by index.

deadCodeInjection: Adds non-functional code to confuse analysis.

Start with moderate settings and increase complexity gradually, testing functionality and performance at each stage. Avoid settings known to break source maps entirely if you need them for production error reporting (though obfuscation often hinders this).

Step 3: Build and Test: Run the production build. Manually inspect the output JS files to confirm they are significantly less readable (mangled names, complex control flow). Perform thorough functional testing of the obfuscated client – log in, play a full match, test all UI interactions, movement, shooting. Ensure nothing is broken by the obfuscation process.

Why: Makes the client's source code much harder for humans to read and understand by renaming variables, obscuring logic flow, and hiding literal strings. This deters casual reverse engineering attempts aimed at finding cheats (e.g., how health is stored, how network messages are handled).

Tip: Obfuscation is not encryption and can be reversed by skilled individuals with enough effort. It's a deterrent, not a foolproof protection. Combine with other techniques.

Unit Test: Compare the output JS bundle size and structure before and after enabling obfuscation. Perform a full regression test suite (manual or automated) on the obfuscated build to confirm no functionality regressions were introduced. Check browser console for errors specific to the obfuscated code.

Troubleshooting:

Obfuscated code breaks functionality: Reduce obfuscation strength/complexity. Disable specific options (like aggressive control flow flattening) one by one to isolate the cause. Check the obfuscator's documentation for known issues or compatibility problems.

Performance degradation: Some obfuscation techniques (especially control flow flattening) can impact runtime performance. Profile the obfuscated code's execution speed in the browser and adjust settings if necessary.

STOP & CHECK: Confirm client-side JavaScript code is successfully obfuscated in production builds, making it significantly harder to read, without introducing functional bugs or unacceptable performance hits.

6.1.2 Memory tamper detection (Basic Checks)

Step 1: Identify Critical Client Variables: Pinpoint client-side JS variables that might be targeted, even though server validation is key. Examples: Local copy of health (for UI), ammo count display, potentially cached speed modifiers (if client uses them for prediction before server correction). Recommendation: Minimize client-side storage of state that affects gameplay logic; rely on server state where possible.

Step 2: Implement "Checksum" Checks (Conceptual):

Periodically (e.g., every few seconds within the game loop), calculate a simple checksum or hash based on a set of related, validated variables. Example: checksum = localHealth * 31 + localArmor * 17;.

Store the expected checksum calculated based on the last known valid state received from the server.

Compare the newly calculated checksum with the expected one.

Alternative: Store redundant copies of critical variables (localHealth1, localHealth2). Check if localHealth1 === localHealth2. Tampering might modify only one copy.

Step 3: Define Detection Action: If a check fails (checksum mismatch, redundant copies differ):

Log the discrepancy locally with variable names and values.

Send a specific, low-priority 'tamper_detected' message to the server with details. socket.emit(MessageType.TAMPER_DETECTED, { checkType: 'healthChecksumMismatch', expected: expectedChecksum, actual: currentChecksum });.

Recommendation: Do not immediately disconnect the client. False positives are possible. Use this as a server-side flag for further investigation or correlation with other suspicious behavior.

Step 4: Obfuscate Checks: Ensure the checking logic itself (calculation, comparison, reporting) is part of the obfuscated codebase (6.1.1) to make it harder to find and disable.

Why: Provides a very basic layer attempting to detect crude memory modification via browser developer tools or external trainers. It signals potential tampering to the server. Limitation: Sophisticated cheats can find and bypass these checks or modify values consistently. Server validation remains paramount.

Unit Test: In the browser's developer console, manually change the value of a monitored variable (e.g., game.localHealth = 999;). Step through the game loop or wait for the next check interval. Verify the detection logic triggers (check console logs) and sends the TAMPER_DETECTED message to the server (check server logs or network tab).

Troubleshooting:

False positives: Ensure checksums/checks only fail due to external modification, not legitimate game state changes between calculation and check. Adjust check frequency or logic. Ensure comparison handles floating-point inaccuracies if applicable.

Checks easily bypassed: This is expected. Focus on making them non-obvious via obfuscation.

STOP & CHECK: Confirm basic client-side checks are implemented to detect naive modifications of critical JS variables and report findings to the server, understanding the inherent limitations.

6.1.3 Input pattern analysis (Heuristics)

Step 1: Collect Input Telemetry (Client): Instrument input handlers (keydown, mousedown, mousemove) to record precise timestamps and event data (key pressed, mouse delta X/Y, click timing). Store a short history (e.g., last 100 events) locally.

Step 2: Develop Heuristic Checks (Client): Periodically analyze the input history buffer for suspicious patterns:

Click Timing: For automatic weapons, calculate the time delta between consecutive 'fire' inputs. Is the variance extremely low (indicative of a script/triggerbot)? const fireIntervals = getFireIntervals(); const variance = calculateVariance(fireIntervals); if (variance < TRIGGERBOT_THRESHOLD) flagSuspicion('lowFireVariance');.

Reaction Time: Measure time between a specific game event (e.g., enemy appears visually - harder to detect reliably) and a corresponding action (e.g., firing). Are reaction times consistently below human limits (< 100-150ms)? (Difficult without reliable event triggers).

Aim Snapping: Analyze mousemove deltas. Are there frequent, large movementX/movementY values immediately followed by near-zero values (indicative of aimbot snapping)? if (Math.abs(deltaX) > SNAP_THRESHOLD && Math.abs(nextDeltaX) < STILL_THRESHOLD) snapCounter++;.

Repetitive Actions: Are certain action sequences repeated with near-identical timing?

Step 3: Server Reporting: If heuristics consistently flag suspicious behavior over a time window (e.g., multiple flags within 10 seconds), send a 'suspicious_input' report to the server with details about the flagged patterns. socket.emit(MessageType.SUSPICIOUS_INPUT, { flags: ['lowFireVariance', 'aimSnapDetected'] });.

Why: Attempts to identify bots and scripts by analyzing if input patterns deviate significantly from expected human behavior. Provides server with additional behavioral signals that can corroborate other detection methods.

Tip: These heuristics are prone to false positives and negatives. Tune thresholds carefully. Use server-side correlation before taking action.

Unit Test: Simulate input sequences: normal human-like clicking/moving (variable timing/paths), perfectly regular clicks, instant mouse movements between two points. Feed these sequences into the heuristic analysis functions. Verify the scripted/bot-like patterns trigger flags while human-like patterns generally do not. Check server logs for received SUSPICIOUS_INPUT reports.

Troubleshooting:

High false positives: Relax thresholds, require more consecutive flags before reporting, improve heuristic logic (e.g., account for weapon fire rate variability).

Not detecting known bots: Analyze the bot's input patterns; refine heuristics to catch them. Add more diverse checks.

STOP & CHECK: Confirm client collects input telemetry, applies heuristic analysis to detect potentially inhuman patterns (like perfect timing or aim snapping), and reports sustained suspicious findings to the server.

6.2 Server Validation

Goal: Implement rigorous checks on the Node.js server, using the authoritative Rapier physics simulation as the ground truth, to detect and prevent cheats definitively. This is the most critical part of anti-cheat.

6.2.1 Movement plausibility checks (Rapier Based)

Step 1: Expand Server Validation Logic: In the server handler for MessageType.PLAYER_INPUT (from 2.3.2), after applying input forces and stepping the server's Rapier world:

Retrieve the resulting authoritative state from the player's Rapier body: const newPos = playerBody.translation(); const newVel = playerBody.linvel();.

Retrieve the state before applying this input (store it temporarily).

Step 2: Speed Check: Calculate the magnitude of the resulting linear velocity newVel.length(). Compare it against the maximum possible speed defined in game config (MAX_PLAYER_SPEED). if (newVel.length() > MAX_PLAYER_SPEED * 1.05 /* Allow small tolerance */) { rejectInput = true; reason = 'Excessive Speed'; }.

Step 3: Noclip/Collision Check: The primary check is already done implicitly by Rapier. If the player tried to move into a wall defined by a static map collider, world.step() would have prevented or adjusted the movement. Add explicit check: Use world.intersectionPair(playerCollider, mapCollider) or check contacts after the step to be absolutely sure the player collider isn't significantly penetrating a static collider.

Step 4: Fly/Teleport Check: Calculate the displacement vector: deltaPos = newPos.sub(previousPos);. Calculate distance moved: distanceMoved = deltaPos.length();. Calculate expected max distance: maxExpectedDistance = MAX_PLAYER_SPEED * inputDeltaTime * 1.1 /* Tolerance */; if (distanceMoved > maxExpectedDistance) { rejectInput = true; reason = 'Impossible Teleport'; }. Check vertical movement against grounded state (use server-side Rapier raycast down to check isGrounded state reliably) - significant upward deltaPos.y when not grounded is suspicious.

Step 5: Input Rejection: If rejectInput is true:

Log the violation with reason, player ID, input data.

Crucially: Do not update the player's authoritative state with the results of this input. Keep their state as it was before processing this packet.

The next GAME_STATE broadcast will send the old, correct state, forcing the cheating client to reconcile back.

Why: Uses the authoritative server physics simulation and defined game rules (max speed) to verify every movement request. Prevents cheats like speed hacks, flying, teleportation, and moving through walls by rejecting any input that results in a physically impossible state according to the server's Rapier world.

Tip: Tune tolerances carefully to avoid flagging legitimate edge cases (e.g., falling speed, slight physics bumps). Log rejected inputs thoroughly for debugging and cheat analysis.

Unit Test: Simulate client sending PLAYER_INPUT packets requesting: movement faster than MAX_PLAYER_SPEED; large jumps in position; upward movement while server knows player isn't grounded; movement directly into a server-side wall collider. Verify the server's validation checks trigger, the input is rejected (log indicates rejection), and the player's authoritative Rapier body state (position, velocity) is not updated according to the invalid input. Check that subsequent GAME_STATE reflects the non-updated state.

Troubleshooting:

Legitimate moves rejected: Check speed limits, tolerances, grounded check logic. Ensure server Rapier simulation matches client prediction reasonably well.

Cheats getting through: Strengthen checks. Verify server collision geometry is accurate. Ensure Rapier world step is correctly simulating the input. Check inputDeltaTime validity.

STOP & CHECK: Confirm the server performs rigorous plausibility checks on movement inputs against Rapier physics results, speed limits, grounded state, and collision geometry, rejecting inputs that lead to impossible states.

6.2.2 Shot verification rewinding (Rapier Based)

Step 1: Consolidate Lag Compensation Logic: Ensure the server-side rewind mechanism from 3.1.3 (storing history, estimating latency, rewinding target Rapier bodies) is robust and reliably executed before any hit validation raycast.

Step 2: Line of Sight (Static Geometry): After rewinding target bodies but before raycasting against hitboxes:

Perform a Rapier raycast from the shooter's (potentially rewound) position (authoritativeOrigin) in the authoritativeDirection.

Set the raycast filter flags/groups to intersect only with static map geometry colliders.

Set the ray's max distance (toi) to the distance to the claimed hit location (if client provided, otherwise distance to target's rewound position).

const lineOfSightHit = world.castRay(staticCheckRay, distanceToTarget, true, /* filter for static map */);.

If lineOfSightHit is not null (meaning it hit static map geometry first), then the shot is impossible. Reject the shot, log "Line of Sight Blocked".

Step 3: Rate of Fire / Ammo Check: Before any raycasting, check server's authoritative state for the player: if (Date.now() - player.serverLastFireTime < WEAPONS[player.weapon].fireRate) { rejectShot = true; reason = 'Rate of Fire Exceeded'; }. Also check if (player.ammo <= 0) { rejectShot = true; reason = 'Out of Ammo'; }. Update serverLastFireTime and decrement ammo only if shot is validated later.

Step 4: Angle / Field of View Check: Calculate the angle between the shooter's authoritative forward direction (at fire time) and the vector pointing from the shooter to the hit location on the target's rewound hitbox (from the main raycast result). const angle = shooterForwardVector.angleTo(vectorToHitPoint); if (angle > MAX_AIM_ANGLE_DEVIATION /* e.g., 90 degrees */) { rejectShot = true; reason = 'Impossible Hit Angle'; }. This helps catch silent aim / 360-degree aimbots.

Step 5: Hitbox Penetration Check (Advanced): If a shot hits, check if the hit point is physically plausible (e.g., not hitting the back of a hitbox from the front). Use the hit normal (hit.normal) and ray direction.

Step 6: Rejection Logic: If any validation check (rejectShot) fails, log the reason and do not proceed with damage calculation (3.2). Do not send HIT_CONFIRMED.

Why: Adds crucial layers of server-side validation beyond simple lag-compensated hitbox checks. Ensures hits are physically possible (line of sight), adhere to game rules (fire rate, ammo), and conform to plausible player aiming capabilities, catching wallhacks, rate-of-fire hacks, and some aimbots.

Tip: Server-side validation is computationally expensive (multiple raycasts, history lookups). Profile this section carefully under load. Optimize queries and validation logic.

Unit Test: Simulate client firing while server knows client is behind static wall; verify Line of Sight check rejects hit. Simulate firing faster than rate limit; verify rejection. Simulate hitting target directly behind shooter; verify angle check rejects hit. Ensure valid shots pass all checks and proceed to damage.

Troubleshooting:

Legitimate shots rejected: Check LoS raycast filters (ensure it ONLY hits static map). Check fire rate/ammo logic synchronization. Check angle calculation and threshold. Ensure rewind logic is accurate.

Cheats still registering hits: Strengthen validation. Check hitbox positions/sizes. Ensure lag compensation isn't overly favouring shooter. Log detailed reasons for hit confirmations/rejections.

STOP & CHECK: Confirm server shot verification includes lag compensation, line-of-sight checks against static geometry, rate-of-fire/ammo checks, and plausible angle checks before confirming a hit.

6.2.3 Rate limiting per action

Step 1: Identify Actions and Limits (Server): Define actions triggered by client network messages (PLAYER_FIRE, PLAYER_JUMP, SEND_CHAT, USE_ABILITY, etc.). Define minimum time interval between valid executions for each in server config (e.g., ACTION_RATE_LIMITS = { PLAYER_FIRE: 50, // ms - minimum, actual weapon rate checked elsewhere PLAYER_JUMP: 500, SEND_CHAT: 1000 };).

Step 2: Track Last Action Time (Server): In the server-side player state object, add a map or object to store the timestamp of the last successful execution of each rate-limited action: player.lastActionTimes = { PLAYER_JUMP: 0, SEND_CHAT: 0 };.

Step 3: Implement Check (Server): At the very beginning of the server handler for each rate-limited action message:

Get current server time now = Date.now();.

Get last execution time: lastTime = player.lastActionTimes[messageType];.

Get limit: limit = ACTION_RATE_LIMITS[messageType];.

Check: if (now - lastTime < limit) { console.warn(\Rate limit exceeded for ${messageType} by player ${player.id}`); return; // Ignore the message entirely }`.

Step 4: Update Timestamp: If the rate limit check passes and the action is successfully processed, update the timestamp: player.lastActionTimes[messageType] = now;.

Why: Provides a generic, server-side defense against spamming any action faster than reasonably possible, whether due to malicious scripts, network manipulation, or client bugs. Prevents simple denial-of-service on action handlers and catches basic automation.

Tip: Set limits reasonably – slightly more permissive than the absolute minimum required by game mechanics – to avoid penalizing players with high latency or minor clock skew. Log violations for monitoring.

Unit Test: Define rate limit for 'PLAYER_JUMP' (e.g., 500ms). Simulate client sending jump messages every 100ms. Verify server logs processing the first, logs rate limit warnings for the next few, processes another after 500ms has passed. Check player.lastActionTimes updates correctly only after successful processing.

Troubleshooting:

Legitimate actions blocked: Check limit values, ensure Date.now() provides appropriate resolution, check timestamp update logic.

Spam still possible: Ensure check happens before any significant processing in the handler. Review limit values.

STOP & CHECK: Confirm server enforces configurable rate limits on key client-triggered actions by checking time elapsed since last successful execution, rejecting messages that arrive too frequently.

6.3 Monitoring

Goal: Observe game activity and player behavior to identify cheating patterns, gather evidence for enforcement, and understand emerging threats.

6.3.1 Cheat pattern detection (Statistical Heuristics)

Step 1: Data Collection (Server): Log detailed event data from authoritative server actions to a persistent store (e.g., specialized analytics DB like ClickHouse, or structured logs sent to Elasticsearch/Splunk, or files for batch processing). Key data:

Kill events (timestamp, killerId, victimId, weapon, distance, killerPos, victimPos, wasHeadshot).

Shot events (timestamp, shooterId, weapon, didHit, hitColliderPart, targetId if hit).

Player position snapshots (timestamp, playerId, posX, posY, posZ) at lower frequency (e.g., 1Hz).

Flag events from client/server validation (timestamp, playerId, flagType, details).

Step 2: Define Heuristics/Metrics: Define statistical metrics calculated over windows (e.g., per match, per hour, per day) for each player:

Headshot Ratio (Headshot Kills / Total Kills).

Accuracy (Hits / Shots Fired) per weapon, potentially bucketed by distance.

K/D Ratio.

Movement Speed Analysis (Average speed, max speed achieved).

Frequency of anti-cheat flags triggered.

Step 3: Implement Analysis Job: Create a background process (scheduled job, serverless function) that periodically queries the collected event data, calculates these metrics per player, and compares them against predefined thresholds or statistical norms (e.g., Z-score compared to player base average).

Example Threshold: if (player.headshotRatio > 0.8 && player.totalKills > 20) flag('Suspiciously High Headshot Ratio');.

Example Z-score: if (calculateZScore(player.accuracy) > 3.0) flag('Accuracy Statistically Unlikely');.

Step 4: Flagging & Reporting: When heuristics flag a player:

Store the flag with evidence (metric value, threshold, time window) in a dedicated 'review' database or ticketing system.

Generate reports/dashboards for human moderators showing flagged players ranked by suspicion score or flag frequency.

Step 5: Model Refinement (Long Term): Use confirmed cheating cases (from manual review) to refine thresholds or train simple machine learning models (e.g., logistic regression, decision trees) on the collected metrics to improve detection accuracy. (ML is a significant step up in complexity).

Why: Moves beyond real-time checks to detect cheats based on abnormal performance patterns over time. Can catch subtle cheats or players trying to hide their cheating. Provides data-driven leads for manual investigation.

Tip: Start with simple, robust heuristics. Focus on metrics less prone to natural skill variance initially (e.g., impossible stats are better flags than just 'high K/D'). Avoid auto-banning based solely on heuristics due to false positives.

Unit Test: Generate sample event data representing normal play and cheated play (e.g., 95% headshots). Run the analysis job. Verify it correctly calculates metrics from the data. Verify the heuristics flag the cheated data based on thresholds. Check the output flags/reports contain the correct player IDs and reasons.

Troubleshooting:

Data collection issues: Verify events logged correctly with all needed fields. Ensure data pipeline is reliable.

Incorrect metrics: Double-check calculation logic (beware division by zero, etc.).

High false positives/negatives: Adjust heuristic thresholds based on analysis of flagged cases vs actual cheaters/non-cheaters. Improve metric definitions.

STOP & CHECK: Confirm detailed gameplay event data is collected server-side and analyzed periodically to calculate statistical metrics, flagging players with highly abnormal performance patterns for review.

6.3.2 Player report system

Step 1: Client UI Implementation: Add a button/option in the game UI (e.g., on the scoreboard screen, post-match screen) allowing players to report another player currently or recently in their match. Present categories (required): Aimbot, Wallhack, Speedhack, Griefing, Other. Add optional text input field.

Step 2: Client Send Report Logic: When player submits report, collect: reporterPlayerId, reportedPlayerId, matchId (crucial for context), timestamp, category, optionalText. Send this data via a specific Socket.io message (MessageType.SUBMIT_REPORT) or a dedicated HTTP POST request to a backend API endpoint.

Step 3: Server Report Handling: Create a backend service/API endpoint to receive reports.

Validate the input data (are IDs valid? is matchId known?).

Store the report securely in a dedicated database table/collection (e.g., player_reports). Schema: reportId, reporterId, reportedId, matchId, timestamp, category, text, status ('new', 'investigating', 'resolved'), resolutionNotes.

Step 4: Moderator Review Tool: Develop a simple web-based internal tool (or use an existing admin panel framework) for moderators.

List incoming reports, sortable/filterable by reported player, status, date, etc.

Aggregate reports against the same player. Display report count per player.

Allow moderators to view report details (including text).

Link reports to relevant match history (4.3.2) and potentially match replays (6.3.3) using matchId.

Allow moderators to update report status and add resolution notes (e.g., "Confirmed cheating, banned", "False report", "Insufficient evidence").

Why: Empowers the community to help identify cheaters. Aggregated reports provide strong signals for investigation. Provides moderators with a structured workflow and context (match data, replays) for reviewing suspicious players.

Tip: Take reports seriously but investigate thoroughly. False reports are common. Look for patterns and corroborating evidence (statistics, replays). Provide feedback to reporters if possible (e.g., "Action was taken based on your report").

Unit Test: Use client UI to submit a report. Verify the SUBMIT_REPORT message sent or API call made with correct data. Check server logs for reception. Check player_reports database; verify report stored correctly. Submit multiple reports for same player; verify moderator tool aggregates them. Test filtering/sorting in moderator tool.

Troubleshooting:

Reports not saving: Check API endpoint/message handler logic. Check database connection/write permissions. Validate incoming data format.

Moderator tool issues: Check tool's database query logic, UI rendering.

STOP & CHECK: Confirm players can submit cheat reports via the client UI, reports are stored persistently on the backend, and a tool exists for moderators to review aggregated reports with relevant context.

6.3.3 Match replay storage (Input/State Recording)

Step 1: Data to Record (Server): During a match instance, record:

Initial State: Map name, player IDs, starting positions/orientations, initial game state.

Player Inputs: All PLAYER_INPUT messages received from both clients, timestamped with server time. Store sequence number, key states, look direction, delta time.

Key Game Events: Timestamped events like PLAYER_FIRE (including origin/direction), DAMAGE_APPLIED (shooter, victim, amount, hitbox), PLAYER_DIED, PLAYER_SPAWNED, ROUND_START, ROUND_END.

(Optional) Periodic State Snapshots: Less frequent (e.g., every 0.5-1s) snapshots of authoritative Rapier body states (pos, rot, vel) for all relevant entities. Useful for scrubbing/resyncing replays, but increases data size. (Focus on inputs + key events first).

Step 2: Recording Mechanism (Server): In the game instance process, append this data chronologically to an in-memory buffer or temporary file during the match.

Step 3: Store Replay Data: When the match ends (endMatch), serialize the recorded data (e.g., into JSON or a compact binary format) and upload it to persistent, cheap storage (e.g., AWS S3, Google Cloud Storage). Store the replay file URL/key alongside the match details in the MongoDB match history (4.3.2). Implement data retention policy (delete replays older than X days).

Step 4: Replay Viewer (Client or Separate Tool): Develop logic to:

Fetch the replay data file for a given matchId.

Initialize a local game environment (Three.js scene, Rapier world with correct map).

Instantiate player representations.

Process the recorded data chronologically:

Apply recorded PLAYER_INPUT messages to the corresponding player's Rapier body simulation (using the recorded delta times).

Step the local Rapier world at the original server tick rate (requires careful timing reconstruction).

Trigger visual effects/state changes based on recorded game events (PLAYER_FIRE, DAMAGE_APPLIED, etc.).

(If using state snapshots, periodically resync local state to snapshot state).

Update Three.js visuals based on the reconstructed Rapier state each frame.

Step 5: Replay Controls: Add playback controls (play/pause, speed control, potentially timeline scrubbing if state snapshots are recorded). Allow switching camera perspective (player 1 FPV, player 2 FPV, free camera).

Why: Provides an objective recording of exactly what happened during a match based on server-received inputs and events. Essential tool for moderators to definitively verify cheating reports (seeing exactly what a suspected cheater saw and did). Valuable for players to review gameplay.

Tip: Input recording is generally smaller than state snapshot recording but requires deterministic physics (Rapier is generally deterministic if using fixed timestep) and careful re-simulation. Start simple; add features like scrubbing later. Ensure recorded data format is versioned in case of changes.

Unit Test: Play a short match; verify replay data file is generated and stored in S3/GCS. Verify the URL/key is linked in the match history DB. Use the replay viewer to load the file. Verify it visually reconstructs the match accurately - players move as expected, shots happen at right times, deaths occur correctly. Test playback controls. Check file sizes.

Troubleshooting:

Replay inaccurate/desynced: Ensure physics simulation in viewer uses exactly the same Rapier settings and timestep as the server did. Verify all necessary inputs/events are recorded and processed in correct order with correct timing. Floating point determinism can be tricky across different runs/machines if not careful.

Replay data missing/corrupt: Check recording logic, serialization, upload process, fetching process.

Viewer performance poor: Optimize re-simulation loop, rendering. Consider loading data progressively.

STOP & CHECK: Confirm server records inputs and key events during matches, stores this replay data persistently, and a viewer tool can accurately reconstruct the match visually from this data for review.

Phase 7: Polish & Launch

Objective: Ship a stable, performant, engaging, and scalable production-ready 1v1 FPS duel experience.

7.1 Performance Tuning

Goal: Optimize client and server performance for a smooth, responsive experience under real-world conditions.

7.1.1 Client FPS optimization (JS / Three.js / Rapier)

Step 1: Establish Performance Baseline: Define target minimum and recommended hardware specs. Measure FPS (Frames Per Second) and frame time consistency on these specs during typical 1v1 gameplay (movement, shooting, effects). Use browser developer tools (Performance tab for CPU/JS profiling, potentially GPU profiling if available) and external tools (e.g., OS frame rate counters, graphics driver overlays).

Step 2: Identify Bottlenecks: Analyze profiles. Is it CPU-bound (long JS execution times, garbage collection pauses, heavy physics calculation if client Rapier step is complex) or GPU-bound (high draw calls, complex shaders, texture bandwidth, vertex count)?

Step 3: CPU/JS Optimization:

Game Loop: Optimize calculations within requestAnimationFrame. Cache frequently accessed values. Avoid unnecessary object creation/destruction (garbage collection pressure).

DOM Updates: Minimize direct DOM manipulation for HUD/UI updates; batch updates or use efficient patterns.

Physics: Profile client-side world.step() time. If significant, consider simplifying client-side prediction colliders (if different from visual models) or reducing physics step frequency (can impact prediction accuracy).

Web Workers: Offload truly heavy, non-rendering computations (e.g., complex AI if added later, pathfinding) to Web Workers to avoid blocking the main thread.

Step 4: GPU/Rendering Optimization (Three.js):

Draw Calls: Use InstancedMesh for identical repeated objects (particles, debris). Merge static geometry where possible (BufferGeometryUtils.mergeBufferGeometries).

Level of Detail (LOD): Ensure LOD system (5.2.3) is working effectively and thresholds are well-tuned.

Materials/Shaders: Use simpler materials (e.g., MeshBasicMaterial or MeshLambertMaterial if full PBR of MeshStandardMaterial isn't needed) for distant or insignificant objects. Optimize custom shader code.

Textures: Use compressed texture formats (Basis Universal via KTX2Loader, or DDS/ETC/PVRTC). Ensure textures are sized appropriately (power-of-two dimensions often preferred). Mipmapping is crucial.

Shadows: Optimize shadow map resolution (shadow.mapSize) and shadow camera frustum size – make it as tight as possible around the dynamic scene. Consider cheaper shadow map types.

Post-Processing: Effects like SSAO, Bloom can be expensive. Profile their cost and make them optional graphics settings.

Frustum Culling: Ensure Three.js's automatic frustum culling is working (objects outside camera view aren't rendered). Add manual occlusion culling if needed for complex indoor scenes (advanced).

Step 5: Iterate and Measure: Implement optimizations one by one. Re-profile and measure FPS/frame times after each change to confirm improvement and ensure no regressions were introduced. Aim for stable frame times, not just high average FPS.

Why: A smooth, high frame rate is critical for the perceived quality and responsiveness of an FPS game. Optimization ensures the game is accessible and enjoyable on a wider range of hardware.

Tip: Profile early, profile often. Optimize based on data, not guesswork. Focus on the biggest bottlenecks first. Provide graphics quality settings for users.

Unit Test: Define FPS/frame time targets for min/rec specs. Run baseline test. Apply optimization (e.g., implement instancing). Run test again. Verify metrics improved towards the target. Use profiling tools to confirm bottleneck was addressed (e.g., draw calls reduced).

Troubleshooting:

Optimization doesn't improve FPS: The bottleneck might be elsewhere. Re-analyze profiles. The optimization might have unintended side effects.

Visuals break after optimization: Double-check implementation (e.g., instancing attributes, merged geometry UVs, LOD switching logic).

STOP & CHECK: Confirm client-side performance has been profiled on target hardware, key CPU/GPU bottlenecks identified, and optimizations implemented (LOD, instancing, material/texture optimization, JS tuning) to achieve stable target frame rates.

7.1.2 Server tick rate calibration (Node.js / Rapier)

Step 1: Define Target Tick Rate: Choose desired rate based on gameplay needs (e.g., 30Hz, 60Hz). Higher = smoother simulation, lower = less CPU load. 30-60Hz is common. const SERVER_TICK_RATE_HZ = 60; const TICK_INTERVAL_MS = 1000 / SERVER_TICK_RATE_HZ;.

Step 2: Implement Precise Loop: Use a precise timer loop for the server's main update function (which includes input processing, world.step(), state broadcasting). Avoid simple setInterval due to drift. A common pattern: let lastTickTime = performance.now(); function serverLoop() { const now = performance.now(); const deltaTime = (now - lastTickTime) / 1000; processInputs(deltaTime); world.step(); // Step Rapier world updateGameStates(deltaTime); broadcastUpdates(); lastTickTime = now; const timeTaken = performance.now() - now; const sleepTime = TICK_INTERVAL_MS - timeTaken; setTimeout(serverLoop, Math.max(0, sleepTime)); // Schedule next tick } setTimeout(serverLoop, TICK_INTERVAL_MS);.

Step 3: Profile Tick Duration: Instrument the serverLoop to measure the timeTaken for each full tick. Log these durations, especially under load (use process.hrtime() for higher precision timing).

Step 4: Analyze Under Load: Run the server game instance with the maximum number of players (2 for 1v1) engaging in heavy activity (constant movement, shooting). Monitor the logged timeTaken.

Step 5: Calibrate:

If timeTaken consistently exceeds TICK_INTERVAL_MS, the server cannot keep up. Action: Lower SERVER_TICK_RATE_HZ OR optimize the code within the loop (especially world.step() configuration, input processing, state update logic).

If timeTaken is consistently much lower than TICK_INTERVAL_MS (e.g., < 50%), consider if a higher tick rate is feasible and beneficial, balancing CPU cost vs simulation fidelity.

Step 6: Optimize Server Loop Code: Profile the Node.js process using built-in (--prof) or external profilers (e.g., Clinic.js, Node Application Metrics Dashboard) under load. Identify slow functions within the tick loop: Is world.step() taking too long (check Rapier settings like substeps)? Is input processing or state serialization inefficient? Optimize hotspots. Ensure all I/O (DB access, etc.) is asynchronous and doesn't block the event loop.

Why: Ensures the server's authoritative simulation runs consistently and on time. A stable tick rate is vital for predictable physics, fair hit registration, and smooth networked experience. Failure to keep up results in server-side lag.

Tip: Node.js is single-threaded for JS execution. Long-running synchronous operations in the tick loop will block everything. Keep the tick loop lean and fast. Use asynchronous operations for I/O.

Unit Test: Configure target tick rate. Run server under simulated max load. Log tick durations over several minutes. Verify average duration is below TICK_INTERVAL_MS and variance is low (few spikes). Use profiler to confirm no single function dominates tick time excessively. Test changing tick rate and re-measure performance/CPU usage.

Troubleshooting:

Tick duration exceeds interval: Profile server loop; identify bottlenecks (Rapier step? JS logic? GC?). Optimize or lower tick rate.

Inconsistent tick times (jitter): Look for blocking operations, garbage collection pauses (check memory usage), external system latency impacting async operations within the loop.

STOP & CHECK: Confirm the server tick rate is calibrated such that the main game loop consistently completes within the allocated time per tick under maximum expected load per instance, ensuring simulation stability.

7.1.3 Database indexing (Review)

Step 1: Review Query Patterns: Re-examine all database queries used by backend services (matchmaking, player profiles API, results service, leaderboard aggregation) now that features are more complete. Use application logs or database slow query logs to identify frequently executed or slow queries.

Step 2: Analyze Query Plans (MongoDB): For critical MongoDB queries, use db.collection.find(...).explain("executionStats"). Look for stages like COLLSCAN (collection scan - bad) or IXSCAN (index scan - good). Check nReturned vs totalDocsExamined. High totalDocsExamined indicates inefficient indexing.

Step 3: Add/Optimize Indexes (MongoDB): Based on analysis, add missing indexes (createIndex) or modify existing ones (e.g., create compound indexes for queries filtering/sorting on multiple fields). Ensure indexes cover the query predicates and sort orders.

Step 4: Analyze Redis Usage: Review Redis commands used. Ensure optimal data structures are employed (Hashes for lookups, Sets for membership, Sorted Sets for ranking/range queries). Avoid slow commands like KEYS in production code. Check Redis latency metrics if available (e.g., redis-cli --latency).

Why: As the application evolves, query patterns change. Re-evaluating database performance and indexing during the polish phase ensures supporting services remain fast and scalable under production load, preventing bottlenecks outside the core game simulation.

Tip: Indexing is an ongoing process. Monitor database performance after launch and adjust indexes as needed based on real-world usage patterns.

Unit Test: Identify a key query (e.g., fetch player match history). Run explain() before optimization; note execution time and plan (e.g., COLLSCAN). Add/optimize index. Run explain() again; verify plan changed to IXSCAN and execution time decreased significantly. Test Redis command performance under simulated load.

Troubleshooting:

Query still slow after indexing: Index might not cover the query optimally (wrong fields, wrong order in compound index). Query logic itself might be inefficient. Data volume might be extremely large, requiring archival or sharding.

Indexes increase write latency: Adding too many indexes can slow down write operations. Find a balance based on read/write patterns.

STOP & CHECK: Confirm database queries for key backend operations (profile lookup, match history, leaderboards, matchmaking) are optimized with appropriate indexes/data structures based on final application usage patterns.

7.2 Progression Systems

Goal: Add features that provide long-term goals and personalization, encouraging player retention. (Implemented using JS server logic and Vanilla JS/DOM client UI).

7.2.1 Player profiles (Refinement)

Step 1: Finalize Profile Data: Decide on the final set of stats and info to store and display (username, ELO, level, XP, K/D, win rate, total matches, longest kill streak, preferred weapon stats, equipped cosmetics). Ensure DB schema (MongoDB) and server state accommodate these.

Step 2: Implement Stat Updates (Server): Ensure robust logic exists (e.g., in the results reporting service) to update player stats in the persistent DB (MongoDB) accurately after each match concludes. Handle edge cases (aborted matches, ties).

Step 3: Backend API Endpoint (/api/player/:playerId/profile): Create/refine the backend HTTP API endpoint (e.g., using Express.js) to fetch profile data for a given player ID from MongoDB. Ensure it only returns data appropriate for public viewing if fetching another player's profile. Add authentication/authorization if needed.

Step 4: Client UI Profile Screen: Create a dedicated section in the client UI (using HTML/CSS, manipulated by vanilla JS).

Fetch profile data using fetch('/api/player/...').

Display stats clearly using DOM manipulation (element.textContent = ...).

Show equipped cosmetics visually (display icons or names).

Allow viewing own profile and potentially basic stats of opponents (e.g., on post-match screen).

Why: Provides players a persistent identity, tracks progress, and showcases achievements/customization, forming the foundation for engagement loops.

Tip: Keep profile data focused. Calculate complex stats (like win rate over last N games) on demand via API logic rather than storing excessive pre-calculated values, unless needed for performance.

Unit Test: Create user, play match, verify stats update in DB via API call. Fetch profile via API; verify correct data returned. Check client UI displays fetched data correctly. Test fetching own vs other player profile (if different data exposed).

Troubleshooting:

Stats incorrect: Check server update logic post-match. Check DB queries in API.

UI display issues: Check client-side JS fetching data, parsing JSON, and updating DOM elements. Check CSS styling.

STOP & CHECK: Confirm players have persistent profiles storing key stats/info, updated reliably post-match, accessible via API, and displayed clearly in the client UI.

7.2.2 Cosmetic unlock system (Implementation)

Step 1: Define Items & Unlock Criteria: Create a definitive list of cosmetic items (weapon skins, etc.) in a config file or database (shared/src/cosmeticsConfig.js or cosmetics DB collection). Define how each is unlocked (player level X, achievement Y, default item). Item definition should include ID, name, type, asset reference (e.g., texture path or model variant identifier).

Step 2: Track Unlocked Items (Server): Add an array field unlockedCosmeticIds to the player profile schema in MongoDB.

Step 3: Granting Logic (Server): Implement server-side functions triggered by events:

onPlayerLevelUp(playerId, newLevel): Check cosmeticsConfig for items unlocked at newLevel; add their IDs to player's unlockedCosmeticIds in DB if not already present.

onAchievementComplete(playerId, achievementId): Check config for cosmetic reward linked to achievementId; grant it.

Step 4: Track Equipped Items (Server): Add equippedCosmetics = { weaponSkin: 'defaultPistolSkinId', ... } object to player profile schema to store currently selected items per slot.

Step 5: Customization UI (Client): Create UI screen (HTML/CSS/JS) listing available slots (weapon, character).

Fetch player profile data (including unlockedCosmeticIds and equippedCosmetics).

For each slot, display unlocked items (filter cosmeticsConfig using unlockedCosmeticIds). Show preview if possible. Highlight the currently equipped item.

Allow player to select an unlocked item. On selection, send an API request to the backend to update the player's equippedCosmetics in their profile DB document.

Step 6: Apply Cosmetics In-Game (Client):

When loading player models/weapons, fetch the player's (own or remote) equippedCosmetics from their profile data (likely received via GAME_STATE or initial match setup).

Modify the Three.js loading/setup logic: If an equippedSkinId is present, load the corresponding texture/material variant instead of the default one. Example: const skinTexture = loadTexture(COSMETICS[equippedSkinId].texturePath); weaponMaterial.map = skinTexture; weaponMaterial.needsUpdate = true;.

Why: Adds personalization and long-term collection goals, driving engagement without impacting gameplay balance.

Tip: Start with a simple system (e.g., weapon skins unlocked by level). Keep asset loading efficient; don't load all possible cosmetics at once.

Unit Test: Define cosmetic unlocked at Level 2. Simulate player reaching Level 2; verify item ID added to unlockedCosmeticIds in DB. Fetch profile; verify unlock. Use client UI; verify item appears as unlocked. Equip item; verify API call updates equippedCosmetics in DB. Start match; verify the client loads and applies the correct equipped skin texture/model visually for both local and remote players based on their profile data.

Troubleshooting:

Unlocks not granted: Check server triggering logic (level up, achievement complete). Check DB update call.

UI doesn't show unlocks: Check client fetch logic, filtering of config vs unlocked IDs.

Equipped item not saved/applied: Check API endpoint for saving equipped choice. Check in-game logic correctly reads equipped data and modifies material/model loading. Ensure asset paths in config are correct.

STOP & CHECK: Confirm system allows unlocking cosmetics via defined criteria, players can manage/equip unlocked items via UI, choices are saved persistently, and equipped cosmetics are visually applied in-game.

7.2.3 Achievement tracking (Implementation)

Step 1: Define Achievements: Finalize list of achievements (ID, name, description, criteria, rewards like XP or specific cosmetic IDs) in config (shared/src/achievementsConfig.js or DB collection). Criteria need to be specific and measurable (e.g., { event: 'kill', count: 100 }, { event: 'win_match', filter: { gameMode: '1v1_duel' }, count: 10 }, { event: 'headshot_kill', count: 50 }).

Step 2: Track Progress (Server): Add achievementsProgress = { achievementId1: { currentCount: 5 }, achievementId2: { completed: true, timestamp: ... } } object to player profile schema in MongoDB.

Step 3: Event Listeners (Server): Implement server logic to listen for relevant game events as they happen or process them post-match from recorded data:

handleKillEvent(killer, victim, weapon, isHeadshot, distance): Check achievementsConfig for achievements related to 'kill', 'headshot_kill', specific weapons, etc. If found, increment currentCount for that achievement in the killer's achievementsProgress in DB.

handleMatchEnd(winner, loser, matchData): Check for achievements related to 'win_match', 'play_match', specific map/mode. Increment progress.

Step 4: Check Completion & Grant Rewards (Server): After incrementing progress for an achievement, check if currentCount >= criteria.count. If yes:

Mark achievement as completed in achievementsProgress (set completed: true, add timestamp).

Grant rewards: Add XP to player state, call cosmetic unlock function (from 7.2.2) if cosmetic reward ID is specified.

Send real-time notification to client: socket.emit(MessageType.ACHIEVEMENT_UNLOCKED, { achievementId, name, description });.

Step 5: Client UI Display: Create 'Achievements' screen (HTML/CSS/JS).

Fetch player profile data (including achievementsProgress).

Display all achievements from achievementsConfig. For each, show name, description, criteria (e.g., "5/10 Wins"). Mark completed achievements visually.

Implement listener for ACHIEVEMENT_UNLOCKED message to show a temporary pop-up notification on the HUD.

Why: Provides structured goals and rewards players for various milestones and playstyles, enhancing long-term engagement.

Tip: Track progress incrementally where possible (on event) rather than scanning huge logs post-match for better performance, but ensure atomicity if multiple conditions need updates. Start with simple count-based achievements.

Unit Test: Define achievement "Win 1 match". Simulate player winning a match. Verify handleMatchEnd increments progress, detects completion, marks as complete in DB, grants rewards (check logs/state), and sends notification. Check client UI displays achievement as completed and notification appears. Test progress tracking (e.g., get 3/5 kills for a kill achievement).

Troubleshooting:

Progress not tracking: Check event listeners correctly trigger progress updates. Verify DB updates are saved. Check criteria matching logic.

Completion not detected/reward not granted: Check currentCount >= criteria.count logic. Check reward granting function calls.

Client UI incorrect: Check fetching/display logic for progress and completion status. Check notification listener.

STOP & CHECK: Confirm system tracks player progress towards defined achievements based on game events, marks achievements as complete, grants rewards, notifies the client, and displays progress/completion status in the UI.

7.3 Launch Prep

Goal: Ensure the game, infrastructure, and team are fully prepared for a smooth public launch and ongoing operation.

7.3.1 Load testing (Final Validation @ Target Scale+)

Step 1: Define Target Launch Scale: Determine the realistic target for peak concurrent users (CCU) at launch (e.g., 5,000 CCU = 2,500 concurrent 1v1 matches) and a stress target (e.g., 10,000 CCU = 5,000 matches, aiming for 2x initial goal from plan).

Step 2: Prepare Production-Like Environment: Set up a staging environment that mirrors the production infrastructure exactly (same Kubernetes cluster size/config, database tiers, Redis config, load balancers, regional setup if applicable).

Step 3: Refine Load Test Scripts: Update load testing scripts (from 7.3.1 initial tests) to simulate the full user flow with all features enabled: login -> profile fetch -> queue -> matchmaking -> connect to game instance -> play full match (movement, shooting, physics interaction, round flow, score updates) -> receive results -> view profile/leaderboard -> potentially re-queue. Use realistic timings and action distributions.

Step 4: Execute Sustained Tests: Run load tests for extended durations (e.g., 1-2 hours) at target launch scale (5k CCU) and then at stress scale (10k CCU). Monitor all key metrics dashboards (from 7.3.3) closely throughout the tests.

Step 5: Analyze Results & Identify Bottlenecks: Look for:

Performance degradation over time (memory leaks, resource exhaustion).

Latency increases (API response time, matchmaking time, in-game network latency).

Error rate spikes (application errors, DB errors, network errors).

Resource saturation (CPU, memory, network on servers/DBs/Redis).

Auto-scaling behavior (did it keep up? did it flap?).

Specific feature bottlenecks (e.g., leaderboard query slow under load).

Step 6: Optimize and Retest: Address any identified bottlenecks (code optimization, infrastructure tuning, scaling configuration adjustments). Rerun sustained tests until the system can handle the stress target (10k CCU) smoothly for the full duration with acceptable performance and error rates.

Why: Simulates the actual pressures of launch more accurately than earlier tests. Catches integration issues, performance regressions, and scaling limits under sustained load with the final codebase. Builds confidence in the system's ability to handle launch day traffic.

Tip: Start tests well before launch date to allow time for optimization cycles. Use distributed load generation tools if needed to generate sufficient traffic. Capture detailed logs and metrics during tests.

Unit Test: Define pass criteria for the sustained load test (e.g., <1% error rate, avg matchmaking time < 10s, server tick rate stable +/- 5%, API p95 latency < 200ms) at stress scale (10k CCU). Run the test. Monitor dashboards. Verify all criteria are met for the duration. If not, analyze metrics/logs to find bottleneck, fix, and re-run.

Troubleshooting:

System crashes under load: Analyze logs (application, cluster, database) leading up to crash. Check resource limits. Look for memory leaks.

Performance degrades over time: Suspect resource leaks (memory, connections), inefficient algorithms that scale poorly, or thermal throttling.

Specific service bottleneck: Focus optimization efforts on the component hitting limits first (e.g., database, Redis, specific API).

STOP & CHECK: Confirm the entire system, in a production-like environment, has successfully passed sustained load tests simulating peak launch traffic (e.g., 5k CCU) and stress traffic (e.g., 10k CCU), meeting predefined performance and stability criteria.

7.3.2 Failover systems (Testing & Validation)

Step 1: Identify Critical Failure Points: List all single points of failure or components requiring high availability: Kubernetes control plane (usually managed by cloud provider), database primary node, Redis master node, load balancers, critical backend services (matchmaking, API), regional infrastructure (if multi-region).

Step 2: Verify HA Configurations: Double-check configurations: Are databases/Redis using replicas across multiple Availability Zones (AZs)? Are backend service deployments running multiple replicas across AZs? Is cluster autoscaler enabled? Are DNS health checks configured for regional failover (if applicable)?

Step 3: Plan Chaos Engineering Tests: Design specific tests to simulate failures in the staging environment while running a low-level load test to simulate active users:

Database Failover: Force manual failover of primary DB node; terminate primary DB pod/VM .

Redis Failover: Force manual failover of Redis master; terminate master pod/VM .

Service Pod Failure: Delete multiple pods of a critical service (e.g., matchmaking); verify Kubernetes replaces them and load balancer redirects traffic.

Node Failure: Cordon and drain a Kubernetes node; verify pods reschedule to other nodes.

AZ Failure: Simulate an AZ becoming unavailable (if cloud provider tools allow, or by terminating all nodes in one AZ).

Region Failure (if multi-region): Simulate outage of one region; verify DNS routes traffic to healthy region(s).

Step 4: Execute Tests and Measure Impact: Run each test scenario. Monitor application availability (can users still queue/play?), data consistency (was there data loss?), and recovery time (how long did it take for the system to stabilize?). Observe failover mechanisms in action via logs and dashboards.

Step 5: Refine and Document: Fix any issues uncovered during testing (e.g., failover didn't trigger, recovery was too slow, data inconsistency occurred). Document the failover procedures and expected recovery times.

Why: Ensures that the high availability and redundancy measures actually work under pressure. Builds confidence that the system can withstand common infrastructure failures with minimal disruption to players. Identifies weaknesses in the failover strategy before launch.

Tip: Communicate planned failover tests to the team. Start with less critical components and gradually increase impact. Automate tests if possible. Focus on measuring user impact during failure.

Unit Test: Define success criteria for each failover test (e.g., < 1 minute downtime for DB failover, no data loss, players in unaffected matches continue uninterrupted). Execute test (e.g., terminate primary DB). Monitor system and metrics. Verify success criteria met. If not, investigate, fix config/logic, re-test.

Troubleshooting:

Failover doesn't trigger: Check health check configurations, replication status, failover controller settings (e.g., Redis Sentinel quorum).

Data loss occurs: Review replication configuration (synchronous vs asynchronous), transaction handling during failover.

Application doesn't recover: Check if services correctly handle reconnections to new DB/Redis primaries. Check load balancer health checks removing failed instances.

STOP & CHECK: Confirm high availability configurations for critical components have been rigorously tested by simulating failures, and the system automatically recovers within acceptable downtime and without data loss.

7.3.3 Monitoring dashboards & Alerting (Final Review & Readiness)

Step 1: Review Dashboards: Go through all monitoring dashboards created (Operations, Developer, Business - from Phase 7.3.3 setup). Ensure they are clear, display the most critical metrics prominently, and cover all key system components (frontend, backend APIs, matchmaking, game servers, DBs, Redis, K8s cluster). Verify data is accurate and updating in real-time.

Step 2: Review Alerting Rules: Examine configured alerts. Are thresholds appropriate (not too sensitive, not too insensitive)? Do alerts cover all critical failure conditions (high error rates, resource saturation, service unavailability, high latency, low available server pool, failed jobs)? Are notification channels (PagerDuty, Slack) correctly configured and routed to the right on-call team members?

Step 3: Test Alerts: Manually trigger test alerts or simulate conditions that should trigger alerts (e.g., spike CPU, generate errors, stop a critical service in staging). Verify alerts fire promptly, contain useful information (what's wrong, where, severity), and reach the intended recipients.

Step 4: Document On-Call Procedures (Runbooks): Create simple runbooks or troubleshooting guides linked to specific alerts. What should the on-call person check first? What are common causes? What are the steps to mitigate or resolve? Who should be escalated to if needed?

Step 5: Team Readiness: Ensure the operations/on-call team understands the dashboards, alert meanings, and runbooks. Conduct a brief drill or walkthrough if necessary. Confirm schedules are set.

Why: Ensures the team has the necessary visibility and proactive warnings to manage the system effectively during launch and beyond. Well-configured monitoring and alerting are crucial for detecting and responding to issues quickly, minimizing player impact. Runbooks streamline incident response.

Tip: Prioritize alerts based on user impact. Have clear dashboards for the "golden signals" (Latency, Traffic, Errors, Saturation) for each service. Keep runbooks concise and actionable.

Unit Test: Review specific dashboard panel; verify it accurately reflects system state under test load. Trigger a critical alert condition (e.g., database CPU > 90%); verify alert fires within defined time, notification received via correct channel, alert message is informative. Review runbook for that alert; verify steps are clear and relevant.

Troubleshooting:

Alerts not firing / flapping: Check metric queries, threshold values, evaluation intervals, notification channel integrations.

Dashboards confusing / missing data: Refine dashboard layout, check metric collection agents/exporters, verify data sources.

Runbooks unclear / outdated: Update documentation based on latest system architecture and common issues found during testing.

STOP & CHECK: Confirm monitoring dashboards provide comprehensive system visibility, alerting rules are configured correctly and tested, notification channels work, and basic operational runbooks/procedures are documented and understood by the team.

This completes the detailed breakdown for phases 6 and 7, aiming to provide thorough guidance for development and launch preparation within the specified technical stack.
